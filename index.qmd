---
title: "Intro to Machine Learning Notes"
subtitle: "Introduction to Machine Learning/Statistical Analysis and Learning"
---

## R Setup

```{r}
#| output = FALSE
#Load packages for chapter labs
library("ISLR2") #Introduction to Statistical Learning datasets'
library("tidyverse")
```

## Week 1 Notes

Types of machine learning:

-   Supervised learning

    -   Regression models - quantitative/continous output
    -   Classification models - qualitative/discrete output

-   Unsupervised learning

    -   Clustering models - patterns from input data without specified output

Why study statistical learning?

-   Inference - how does a particular input drive an output variable

-   Prediction - only the value of the outcome variable is of interest

    -   Example: "How much rainfall will California have in 2050?"?

## Chapter 2: Statistical Learning Introduction

### Notes

Shared assumptions of linear and non-linear statistical models for estimating *f*:

-   Parametric vs. non-parametric

-   Flexibility (complexity) vs. interpretability

    -   Flexible models are beneficial for prediction
    -   Interpretable models are beneficial for inference

-   Supervised vs. unsupervised learning

    -   Unsupervised has no specified outcome variable

How to determine the best statistical model for a problem

-   Quality of fit

-   Variance/Bias tradeoff

    -   Variance: the amount by which $\hat{f}$ would change if estimated with a different training set
    -   Bias: the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
    -   In general, with more flexible methods, variance increases and bias decreases

Inference vs. Prediction

-   Sometimes both are of interest, but often only one is of primary interest. The goals of the analyst here are the primary driver in selecting a model

How do we estimate *f*?

-   Training data used to estimate *f*

-   Parametric approach

    -   Step 1: Specify an estimated functional form for *f* (i.e. a linear function)

    -   Step 2: Training data is used to estimate the parameters of the function

    -   Disadvantage of parametric methods: not well suited to estimate a function for a complex dataset

-   Non-parametric approach

    -   Avoids the assumption of a particular functional form for *f*

    -   Disadvantage: they require very large data sets to make an accurate prediction of *f*

Why would we ever choose to use a more restrective method instead of a very flexible approach?

-   Interpretability, inference, to avoid overfitting

Should we always choose a more complex (flexible) approach when prediction is the objective?

-   Only if there is a very large dataset. With a small amount of data, more complexity is not always good

Measuring the Quality of Fit

-   Mean Squared Error (MSE)

    $$
    MSE = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{f}(x_i))^2
    $$

-   Training MSE vs. Test MSE

-   Why does *training* MSE always decrease with added flexibility?

    -   With enough flexibility you can get your model to perfectly fit the training data (but *test* MSE would be much worse)

-   highest quality of fit does not equal the best predictive model (overfitting)

### Lab: Intro to R

`c()` Create a vector of items

`length()` Returns the length of a vector

`ls()` Lists all objects such as data and functions saved in the environment

`rm()` Remove an object from the environment

`matrix(data = , nrow = , ncol =)` Create a matrix

`sqrt()` calculate the square root

`rnorm(n)` Generates a vector of random normal variables of ~n~ sample size

`cor(x, y)` Calculates the correlation between two sets of numbers

`set.seed()` Used to set a consistent seed for a random number function

`mean()` Mean

`var()` Variance

`sd()` Standard Deviation

`plot()` Basic plotting function

`countour()` Creates a countour plot to represent 3D data

```{r}
x <- seq(1,10)
x
x <- 1:10
x
x <- seq(-pi, pi, length = 50)

y <- x
f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))

contour(x, y, f)
contour(x, y, f, nlevels = 45, add = T)
fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
```

`image()` Produces a heatmap plot

`persp()` Creates a 3D plot, arguments `theta` and `phi` control the viewing angles

```{r}
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta = 30)
persp(x, y, fa, theta = 30, phi = 20)
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
```

`dim()` Dimension function returns the number of rows and columns of a matrix

`read.table()` Import data

`write.table()` Export data

Data Frame functions

`data.frame()` Create a data frame

`str()` Used to view a list of variables and first few observations in a data table

`subset()` Used to filter a data table

`order()` Used to return the order of a vector, can sort a data table

`list()` Create a list

## Chapter 3: Linear Regression

### Notes

Inference problem example - which advertising strategy will lead to higher product sales next year?

*Simple linear regression* is a a method for predicting a quantitative response *Y* on the basis of a single predictor variable *X*. A simple model uses the equation:

$$
Y \approx \beta_0 + \beta_1x_1
$$ Residual Sum of Squares (RSS) is the sum of differences between the observed vaues and predicted values

Least squares estimation method minimizes RSS to created an estimation line with the equation above. $\beta_1$ and $\beta_0$ are computable from the predictors and outcomes in the dataset

Standard Errors for OLS estimates

Hypothesis testing steps - if the regression shows a positive or negative sloped line based on the sample, how can we be sure that it is *not* actually a flat line in the population?

1.  estimate parameters and standard errors

2.  calculate t-statistic

3.  Find the corresponding p value

    -   When the t-statistic is large and the p value is low, we can reject the null hypothesis

Accuracy of the model: how well does the model fit the data?

-   *RSE* (Residual Standard Error)

    -   How far on average are the actual outcomes from the prediction line?

-   $R^2$ statistic

    -   A proportional measure always between 0 and 1 that shows how much variation in the data is explained by the model

    -   Can $R^2$ be negative? Technically yes if the model is very bad

-   *F-*statistic

    -   Not about significance but about whether you can reject the null hypothesis for the whole model

Multiple Linear Regression

Important questions in MLR:

1.  Is at least one of the predictors useful in predicting the response? Check with the F-statistic

2.  Do all the predictors help to explain Y, or is only a subset of the predictors useful? Will be covered in Ch. 6

3.  How well does the model fit the data?

4.  Given a set of predictor values, what response value should we predict, and how accurate is our prediction?

Confidence intervals connect the sample variable to the population variable within a certain degree of confidence. A 95% confidence interval says that 95% of random samples will fall within the interval.

Potential problems in MLR:

### Lab: Regression in R

#### Simple Linear Regression

Boston dataset: The outome variable `medv` is median home value by census tract

```{r}
library(MASS)

#view the first 10 observations of the dataset
head(Boston)

#Create a regression equation 
attach(Boston)
lm.fit <- lm(medv ~ lstat)

#View the regression results
lm.fit

#View details about the regression
summary(lm.fit)

#See what is stored in the lm.fit list
names(lm.fit)

#Function to view the coefficients of lm.fit
coef(lm.fit)

#View the confidence interval
confint(lm.fit)

#Generate confidence intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")

#Generate prediction intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")

#plot
plot(lstat, medv)

#Add the least squares line to the plot
abline(lm.fit, lwd = 3, col = "red")

#Use 'col =' to change the color of the points
plot(lstat, medv, col = "red")

#Use 'pch =' to change the shape of the points
plot(lstat, medv, pch = 20)

#Define the point shape directly
plot(lstat, medv, pch = "+")

#Define the point shape with a number
plot(1:20, 1:20, pch = 1:20)


par(mfrow = c(2,2))
plot(lm.fit)

plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))

plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))

```

#### Multiple Linear Regression

```{r}

#Run a regression with specified predictors
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)

#Run a regression on all the predictor variables in the dataset
lm.fit <- lm(formula = medv ~ ., data = Boston)
summary(lm.fit)

library(car)
#Calculate variance inflation factors
vif(lm.fit)

#Run the regression all predictors except one (age) using the "-" sign
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)

#Another way to change the model using update()
lm.fit1 <- update(lm.fit, ~ . - age)


```

#### Interaction terms

There are two ways to include interaction terms in the lm() funtion: $x_1:x_2$ creates an interaction term between the two variables. $x_1 * x_2$ creates an individual variable for each *plus* an interaction term.

($x_1 * x_2$ is shorthand for $x_1 + x_2 + x_1:x_2$)

```{r}
#Run a regression with a predictor variable
summary(lm(medv ~ lstat*age, data = Boston))
```

#### Non-linear transformations on predictors

To transform a variable in `lm()`, use `I()`. For example, to square a predictor you would use `I(x^2)`.

For higher order variables, use the `poly()` function.

```{r}
#Run a regression with a squared predictor term
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)

#Use anova() to see if the quadratic fit is better than the original linear fit
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)

par(mfrow = c(2,2))
plot(lm.fit2)

#Use the poly() function within lm() to generate a regression with higher order polynomials
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)

#Generate a regression with a log-transformed variable
summary(lm(medv ~ log(rm), data = Boston))

```

#### Qualitative predictors

`lm()` automatically generates dummy variables in a regression on a dataset with qualitative variables.

```{r}

#Preview the Carseats dataset with information about carseat sales
str(Carseats)

#A regression on Carseats data with all variables and some interaction terms
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)

#Use contrasts() to view the coding scheme for a qualitative variable
attach(Carseats)
contrasts(ShelveLoc)
```

#### Writing functions in R

```{r}
#Write a function to load relevant libraries
LoadLibraries <- function() {
  library(ISLR2)
  library(MASS)
  print("The libraries have been loaded.")
}


```

## Chapter 4: Classification

### Notes

Classification techniques are used when the dependent variable Y is qualitative or categorical.

1.  Logistic regression
    A.  Estimation process:
        1.  Create probability functions to estimate coefficients based on training data
        2.  Use new probability equation to predict probabilities
        3.  Set a boundary (i.e. .5) to classy to one category or the other
        4.  interpreting coefficients - change in log-odds, not change in probability
    B.  Note: the coefficients estimated in logistic regression are the change in *log-odds*, however, these can be plugged back into an equation to calculate probability
2.  Bayes Classifier
    1.  Calculate probabilities of each classification outcome

    2.  Bayes classifier picks the largest probability to assign a class

    3.  What kind of data is needed to calculate probabilities in step 1? You need a lot of it to make accurate calculations, and sufficient data is not often available. It can, however, be used with simulated data for examples.
3.  Linear Discriminant Analysis (LDA)
    1.  Bayes Theorem - used to flip conditional probabilities
    2.  Assumption - the distributions of X are normal or Gaussian
    3.  Assumption - both the outcome distributions have the same variance (not an assumption of QDA)
    4.  Data statistics needed to plug into the equation with two classes and one predictor: $\sigma, \pi_1, \pi_2, \mu_1, \mu_2$ where $\sigma$ is the shared variance, $\pi_1 and \pi_2$ are the prior probabilities of k = 1, 2, and $\mu_1 and \mu_2$ are the means
4.  Quadratic Discriminant Analysis (QDA)
5.  Naive Bayes

Comparison of methods

### Lab: Classification Models in R

Summary of functions in R: `lm()` Linear regression `glm(...family = binomial)` Logistic regression `lda()` Linear Discriminant Analysis (from the MASS package) `qda()` Quadratic Discriminant Analysis (from the MASS package) `naiveBayes()` Naive Bayes (from the e1071 package)

```{r}
names(Smarket)
dim(Smarket)
summary(Smarket)

cor(Smarket[,-9])

attach(Smarket)
plot(Volume)
```

#### Logistic Regression

The `glm()` function can be used for generalized linear models including logistic regression. Need to include the argument `family = binomial` to specify logistic regression as the method.

```{r}
#Construct a logistic regression model to predict the qualitative variable Direction
glm.fits <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial
)

summary(glm.fits)

#View the coefficients
coef(glm.fits)

#Another way to view the coefficients
summary(glm.fits)$coef

#Access just the p-values for the coefficients
summary(glm.fits)$coef[,4]

#Predict the probability that the market will go up or down
#Set type = response to output probabilities of the form P(Y = 1|X)
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]

#Check whether the probability is of the market going up or down (it is Up)
contrasts(Direction)

#Create a vector of predicted outcomes (up or down)
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"

#Generate a confusion matrix to see the correct and incorrect predictions
table(glm.pred, Direction)
```

When building and testing models it is useful to split the data into *training data* and *test data*. This way you can see well your model will work when exposed to new data.

Step 1: Train the model

```{r}

#Create vector of all observations before 2005
train <- (Year < 2005)
#Use vector to create a dataset of observations only from 2005,
#Boolean vectors can be used to obtain a subset of the rows or columns of a matrix.
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]

#Alternate method of filtering the data
Smarket.2005b <- subset(Smarket, Year >= 2005)

#Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.
glm.fits <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data = Smarket, family = binomial, subset = train
)
```

Step 2: Test the model with new data

```{r}
#Predict direction of the stock market for only the data in 2005
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")

```

Step 3: Compare the predictions to the actual outcomes in the test data

```{r}
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)

#Test set success rate
mean(glm.pred == Direction.2005)
#Test set error rate
mean(glm.pred != Direction.2005)
```

The resulting error rate is not very good but not a surprising result for trying to predict future stock market conditions only based on the last few days. Below the analysis is repeated, but only keeping the predictors Lag 1 and Lag 2 which had the smallest P-values (i.e. higher significance)

```{r}
glm.fits <- glm(
  Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train
)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down",250)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```

Predictions can also be modified to use differenct coefficient values:

```{r}
predict(glm.fits,
        newdata = 
          data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
        type = "response"
)
```

#### Linear Discriminant Analysis

```{r}
library(MASS)
#Set up a Linear Discriminant Analysis model
lda.fit <- lda(
  Direction ~ Lag1 + Lag2, data = Smarket, subset = train
)

lda.fit

plot(lda.fit)

lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)


```

The `predict()` function returns a list with three elements:

1.  `class` LDAâ€™s predictions about the movement of the market

2.  `posterior` a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class

3.  `x` contains the linear discriminants

```{r}
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
```
In this case, LDA produced similar results to Logistic Regression

```{r}
sum(lda.pred$posterior[,1] >= .5)
sum(lda.pred$posterior[,1] < .5)

lda.pred$posterior[1:20,1]
lda.class[1:20]

#Adjust threshold above .5
sum(lda.pred$posterior[,1] > .9)
```


#### Quadratic Discriminant Analysis

```{r}
#Set up a Quadratic Discriminant Analysis model
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda.fit

qda.class <- predict(qda.fit, Smarket.2005)$class

table(qda.class, Direction.2005)

mean(qda.class == Direction.2005)


```

#### Naive Bayes
```{r}
#load library containing the naiveBayes() function
library(e1071)

#Create Naive Bayes model
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
nb.fit

#Use model for prediction
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)

mean(nb.class == Direction.2005)

```
The `predict()` function can also generate estimates of the probability that each observation belongs to a particular class.
```{r}
nb.preds <- predict(nb.fit, Smarket.2005, type = "raw")
nb.preds[1:5,]
```

## Chapter 5: Resampling Methods

### Notes

```{r}
#Test code for github pull
add <- 2+2
add
```

