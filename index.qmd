---
title: "Intro to Machine Learning Notes"
subtitle: "Introduction to Machine Learning/Statistical Analysis and Learning"
---

The lab content on this site is from the textbook [*An Introduction to Statistical Learning with Applications in R, second edition*](https://www.statlearning.com)

## R Setup

```{r}
#| output = FALSE
#Load packages for chapter labs
library("ISLR2") #Introduction to Statistical Learning datasets'
library("tidyverse")
```

## Week 1 Notes

Types of machine learning:

-   Supervised learning

    -   Regression models - quantitative/continous output
    -   Classification models - qualitative/discrete output

-   Unsupervised learning

    -   Clustering models - patterns from input data without specified output

Why study statistical learning?

-   Inference - how does a particular input drive an output variable

-   Prediction - only the value of the outcome variable is of interest

    -   Example: "How much rainfall will California have in 2050?"?

## Chapter 2: Statistical Learning Introduction

### Notes

Shared assumptions of linear and non-linear statistical models for estimating *f*:

-   Parametric vs. non-parametric

-   Flexibility (complexity) vs. interpretability

    -   Flexible models are beneficial for prediction
    -   Interpretable models are beneficial for inference

-   Supervised vs. unsupervised learning

    -   Unsupervised has no specified outcome variable

How to determine the best statistical model for a problem

-   Quality of fit

-   Variance/Bias tradeoff

    -   Variance: the amount by which $\hat{f}$ would change if estimated with a different training set
    -   Bias: the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
    -   In general, with more flexible methods, variance increases and bias decreases

Inference vs. Prediction

-   Sometimes both are of interest, but often only one is of primary interest. The goals of the analyst here are the primary driver in selecting a model

How do we estimate *f*?

-   Training data used to estimate *f*

-   Parametric approach

    -   Step 1: Specify an estimated functional form for *f* (i.e. a linear function)

    -   Step 2: Training data is used to estimate the parameters of the function

    -   Disadvantage of parametric methods: not well suited to estimate a function for a complex dataset

-   Non-parametric approach

    -   Avoids the assumption of a particular functional form for *f*

    -   Disadvantage: they require very large data sets to make an accurate prediction of *f*

Why would we ever choose to use a more restrective method instead of a very flexible approach?

-   Interpretability, inference, to avoid overfitting

Should we always choose a more complex (flexible) approach when prediction is the objective?

-   Only if there is a very large dataset. With a small amount of data, more complexity is not always good

Measuring the Quality of Fit

-   Mean Squared Error (MSE)

    $$
    MSE = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{f}(x_i))^2
    $$

-   Training MSE vs. Test MSE

-   Why does *training* MSE always decrease with added flexibility?

    -   With enough flexibility you can get your model to perfectly fit the training data (but *test* MSE would be much worse)

-   highest quality of fit does not equal the best predictive model (overfitting)

### Lab: Intro to R

`c()` Create a vector of items

`length()` Returns the length of a vector

`ls()` Lists all objects such as data and functions saved in the environment

`rm()` Remove an object from the environment

`matrix(data = , nrow = , ncol =)` Create a matrix

`sqrt()` calculate the square root

`rnorm(n)` Generates a vector of random normal variables of ~n~ sample size

`cor(x, y)` Calculates the correlation between two sets of numbers

`set.seed()` Used to set a consistent seed for a random number function

`mean()` Mean

`var()` Variance

`sd()` Standard Deviation

`plot()` Basic plotting function

`countour()` Creates a countour plot to represent 3D data

```{r}
x <- seq(1,10)
x
x <- 1:10
x
x <- seq(-pi, pi, length = 50)

y <- x
f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))

contour(x, y, f)
contour(x, y, f, nlevels = 45, add = T)
fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
```

`image()` Produces a heatmap plot

`persp()` Creates a 3D plot, arguments `theta` and `phi` control the viewing angles

```{r}
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta = 30)
persp(x, y, fa, theta = 30, phi = 20)
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
```

`dim()` Dimension function returns the number of rows and columns of a matrix

`read.table()` Import data

`write.table()` Export data

Data Frame functions

`data.frame()` Create a data frame

`str()` Used to view a list of variables and first few observations in a data table

`subset()` Used to filter a data table

`order()` Used to return the order of a vector, can sort a data table

`list()` Create a list

## Chapter 3: Linear Regression

### Notes

Inference problem example - which advertising strategy will lead to higher product sales next year?

*Simple linear regression* is a a method for predicting a quantitative response *Y* on the basis of a single predictor variable *X*. A simple model uses the equation:

$$
Y \approx \beta_0 + \beta_1x_1
$$ Residual Sum of Squares (RSS) is the sum of differences between the observed vaues and predicted values

Least squares estimation method minimizes RSS to created an estimation line with the equation above. $\beta_1$ and $\beta_0$ are computable from the predictors and outcomes in the dataset

Standard Errors for OLS estimates

Hypothesis testing steps - if the regression shows a positive or negative sloped line based on the sample, how can we be sure that it is *not* actually a flat line in the population?

1.  estimate parameters and standard errors

2.  calculate t-statistic

3.  Find the corresponding p value

    -   When the t-statistic is large and the p value is low, we can reject the null hypothesis

Accuracy of the model: how well does the model fit the data?

-   *RSE* (Residual Standard Error)

    -   How far on average are the actual outcomes from the prediction line?

-   $R^2$ statistic

    -   A proportional measure always between 0 and 1 that shows how much variation in the data is explained by the model

    -   Can $R^2$ be negative? Technically yes if the model is very bad

-   *F-*statistic

    -   Not about significance but about whether you can reject the null hypothesis for the whole model

Multiple Linear Regression

Important questions in MLR:

1.  Is at least one of the predictors useful in predicting the response? Check with the F-statistic

2.  Do all the predictors help to explain Y, or is only a subset of the predictors useful? Will be covered in Ch. 6

3.  How well does the model fit the data?

4.  Given a set of predictor values, what response value should we predict, and how accurate is our prediction?

Confidence intervals connect the sample variable to the population variable within a certain degree of confidence. A 95% confidence interval says that 95% of random samples will fall within the interval.

Potential problems in MLR:

### Lab: Regression in R

#### Simple Linear Regression

Boston dataset: The outome variable `medv` is median home value by census tract

```{r}
library(MASS)

#view the first 10 observations of the dataset
head(Boston)

#Create a regression equation 
attach(Boston)
lm.fit <- lm(medv ~ lstat)

#View the regression results
lm.fit

#View details about the regression
summary(lm.fit)

#See what is stored in the lm.fit list
names(lm.fit)

#Function to view the coefficients of lm.fit
coef(lm.fit)

#View the confidence interval
confint(lm.fit)

#Generate confidence intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")

#Generate prediction intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")

#plot
plot(lstat, medv)

#Add the least squares line to the plot
abline(lm.fit, lwd = 3, col = "red")

#Use 'col =' to change the color of the points
plot(lstat, medv, col = "red")

#Use 'pch =' to change the shape of the points
plot(lstat, medv, pch = 20)

#Define the point shape directly
plot(lstat, medv, pch = "+")

#Define the point shape with a number
plot(1:20, 1:20, pch = 1:20)


par(mfrow = c(2,2))
plot(lm.fit)

plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))

plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))

```

#### Multiple Linear Regression

```{r}

#Run a regression with specified predictors
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)

#Run a regression on all the predictor variables in the dataset
lm.fit <- lm(formula = medv ~ ., data = Boston)
summary(lm.fit)

library(car)
#Calculate variance inflation factors
vif(lm.fit)

#Run the regression all predictors except one (age) using the "-" sign
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)

#Another way to change the model using update()
lm.fit1 <- update(lm.fit, ~ . - age)


```

#### Interaction terms

There are two ways to include interaction terms in the lm() funtion: $x_1:x_2$ creates an interaction term between the two variables. $x_1 * x_2$ creates an individual variable for each *plus* an interaction term.

($x_1 * x_2$ is shorthand for $x_1 + x_2 + x_1:x_2$)

```{r}
#Run a regression with a predictor variable
summary(lm(medv ~ lstat*age, data = Boston))
```

#### Non-linear transformations on predictors

To transform a variable in `lm()`, use `I()`. For example, to square a predictor you would use `I(x^2)`.

For higher order variables, use the `poly()` function.

```{r}
#Run a regression with a squared predictor term
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)

#Use anova() to see if the quadratic fit is better than the original linear fit
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)

par(mfrow = c(2,2))
plot(lm.fit2)

#Use the poly() function within lm() to generate a regression with higher order polynomials
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)

#Generate a regression with a log-transformed variable
summary(lm(medv ~ log(rm), data = Boston))

```

#### Qualitative predictors

`lm()` automatically generates dummy variables in a regression on a dataset with qualitative variables.

```{r}

#Preview the Carseats dataset with information about carseat sales
str(Carseats)

#A regression on Carseats data with all variables and some interaction terms
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)

#Use contrasts() to view the coding scheme for a qualitative variable
attach(Carseats)
contrasts(ShelveLoc)
```

#### Writing functions in R

```{r}
#Write a function to load relevant libraries
LoadLibraries <- function() {
  library(ISLR2)
  library(MASS)
  print("The libraries have been loaded.")
}


```

## Chapter 4: Classification

### Notes

Classification techniques are used when the dependent variable Y is qualitative or categorical.

1.  Logistic regression
    A.  Estimation process:
        1.  Create probability functions to estimate coefficients based on training data
        2.  Use new probability equation to predict probabilities
        3.  Set a boundary (i.e. .5) to classy to one category or the other
        4.  interpreting coefficients - change in log-odds, not change in probability
    B.  Note: the coefficients estimated in logistic regression are the change in *log-odds*, however, these can be plugged back into an equation to calculate probability
2.  Bayes Classifier
    1.  Calculate probabilities of each classification outcome

    2.  Bayes classifier picks the largest probability to assign a class

    3.  What kind of data is needed to calculate probabilities in step 1? You need a lot of it to make accurate calculations, and sufficient data is not often available. It can, however, be used with simulated data for examples.
3.  Linear Discriminant Analysis (LDA)
    1.  Bayes Theorem - used to flip conditional probabilities
    2.  Assumption - the distributions of X are normal or Gaussian
    3.  Assumption - both the outcome distributions have the same variance (not an assumption of QDA)
    4.  Data statistics needed to plug into the equation with two classes and one predictor: $\sigma, \pi_1, \pi_2, \mu_1, \mu_2$ where $\sigma$ is the shared variance, $\pi_1$ and $\pi_2$ are the prior probabilities of k = 1, 2, and $\mu_1$ and $\mu_2$ are the means
4.  Quadratic Discriminant Analysis (QDA)
5.  Naive Bayes

Comparison of methods

### Lab: Classification Models in R

Summary of functions in R: `lm()` Linear regression `glm(...family = binomial)` Logistic regression `lda()` Linear Discriminant Analysis (from the MASS package) `qda()` Quadratic Discriminant Analysis (from the MASS package) `naiveBayes()` Naive Bayes (from the e1071 package)

```{r}
names(Smarket)
dim(Smarket)
summary(Smarket)

cor(Smarket[,-9])

attach(Smarket)
plot(Volume)
```

#### Logistic Regression

The `glm()` function can be used for generalized linear models including logistic regression. Need to include the argument `family = binomial` to specify logistic regression as the method.

```{r}
#Construct a logistic regression model to predict the qualitative variable Direction
glm.fits <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial
)

summary(glm.fits)

#View the coefficients
coef(glm.fits)

#Another way to view the coefficients
summary(glm.fits)$coef

#Access just the p-values for the coefficients
summary(glm.fits)$coef[,4]

#Predict the probability that the market will go up or down
#Set type = response to output probabilities of the form P(Y = 1|X)
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]

#Check whether the probability is of the market going up or down (it is Up)
contrasts(Direction)

#Create a vector of predicted outcomes (up or down)
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"

#Generate a confusion matrix to see the correct and incorrect predictions
table(glm.pred, Direction)
```

When building and testing models it is useful to split the data into *training data* and *test data*. This way you can see well your model will work when exposed to new data.

Step 1: Train the model

```{r}

#Create vector of all observations before 2005
train <- (Year < 2005)
#Use vector to create a dataset of observations only from 2005,
#Boolean vectors can be used to obtain a subset of the rows or columns of a matrix.
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]

#Alternate method of filtering the data
Smarket.2005b <- subset(Smarket, Year >= 2005)

#Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.
glm.fits <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data = Smarket, family = binomial, subset = train
)
```

Step 2: Test the model with new data

```{r}
#Predict direction of the stock market for only the data in 2005
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")

```

Step 3: Compare the predictions to the actual outcomes in the test data

```{r}
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)

#Test set success rate
mean(glm.pred == Direction.2005)
#Test set error rate
mean(glm.pred != Direction.2005)
```

The resulting error rate is not very good but not a surprising result for trying to predict future stock market conditions only based on the last few days. Below the analysis is repeated, but only keeping the predictors Lag 1 and Lag 2 which had the smallest P-values (i.e. higher significance)

```{r}
glm.fits <- glm(
  Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train
)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down",250)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```

Predictions can also be modified to use differenct coefficient values:

```{r}
predict(glm.fits,
        newdata = 
          data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
        type = "response"
)
```

#### Linear Discriminant Analysis

```{r}
library(MASS)
#Set up a Linear Discriminant Analysis model
lda.fit <- lda(
  Direction ~ Lag1 + Lag2, data = Smarket, subset = train
)

lda.fit

plot(lda.fit)

lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)


```

The `predict()` function returns a list with three elements:

1.  `class` LDA’s predictions about the movement of the market

2.  `posterior` a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class

3.  `x` contains the linear discriminants

```{r}
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
```

In this case, LDA produced similar results to Logistic Regression

```{r}
sum(lda.pred$posterior[,1] >= .5)
sum(lda.pred$posterior[,1] < .5)

lda.pred$posterior[1:20,1]
lda.class[1:20]

#Adjust threshold above .5
sum(lda.pred$posterior[,1] > .9)
```

#### Quadratic Discriminant Analysis

```{r}
#Set up a Quadratic Discriminant Analysis model
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda.fit

qda.class <- predict(qda.fit, Smarket.2005)$class

table(qda.class, Direction.2005)

mean(qda.class == Direction.2005)


```

#### Naive Bayes

```{r}
#load library containing the naiveBayes() function
library(e1071)

#Create Naive Bayes model
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
nb.fit

#Use model for prediction
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)

mean(nb.class == Direction.2005)

```

The `predict()` function can also generate estimates of the probability that each observation belongs to a particular class.

```{r}
nb.preds <- predict(nb.fit, Smarket.2005, type = "raw")
nb.preds[1:5,]
```

## Chapter 5: Resampling Methods

### Notes

Resampling methods: drawing samples from the training data to fit your model multiple times, comparing the goodness of fit across each sample.

-   Model assessment - the process of evaluating a model's performance

-   Model selection - the process of the proper level of flexibility for a model

**Cross-validation**

*Validation Set* approach

1.  If a test dataset is not available, you can split the training data into two sets:

    1.  training set

    2.  validation set

2.  Fit the model using the training set, and then test it on the validation set.

    1.  You can also do this multiple times by using different random selections for the training and validation sets

3.  Creating a validation set is a useful and simple approach but has two drawbacks:

    1.  The validation estimate of the test error can vary substantially depending on how the data is split up

    2.  Only a subset of observations are used to train the model, since some must be withheld for the test/validation set.

*Leave One Out Cross Validation* (LOOCV)

1.  Similar to the validation set approach, but instead of splitting the observations in half, simply split one observation off for validation and use the rest (n-1) for the training set

2.  Repeat the procedure *n* times, calculating the MSE each time

3.  LOOCV estimate is the mean of all the MSEs

    1.  Central limit theorem does not apply since there are correlations between your samples

4.  Downside - has to be fit *n* times which can be time consuming or expensive

    1.  A special rule allows a shortcut for linear or polynomial regression, needing only one fit

*k-Fold Cross Validation*

1.  Divide the observations into *k* folds of equal size, using one for validation and the rest for training
2.  Repeat the procedure *k* times using a different fold as the validation set, recording MSE
3.  Take the average of all the MSEs
4.  Typically performed using *k* = 5 or *k* = 10

Cross validation with classification

1.  Same as above, but instead of using MSE to quantify test error, use the number of misclassified observations

2.  

**Bootstrap**

### Lab: Cross Validation and the Bootstrap

#### Validation Set Approach

```{r}
library(ISLR2)
set.seed(1)

#Split data in two for training and testing
train <- sample(392, 196)

lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)

#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)

#Fit a quadratic and cubic model with the poly() function
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)

lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg- predict(lm.fit3, Auto))[-train]^2)

```

Analysis of the MSe for each model indicates that a quadratic function is an improvement in the linear model, but a cubic model does not offer any significant additional improvement.

#### Leave One Out Cross Validation

You can perform LOOCV using `cv.glm()` For linear regression, instead of using `lm()`, use the `glm()` function but leave the `family` argument blank.

```{r}
#boot package contains cv.glm()
library(boot)

glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

`$delta` contains the CV results The first numbers is the standard CV estimate; the second is a bias-corrected version. In this case they are nearly the same.

To repeat the process of cross validation over multiple polynomial fits, use a for loop:

```{r}
cv.error <- rep(0,10)

for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}

#Display the test MSE for each all polynomial models
cv.error
```

There is a decrease in test MSE from linear to quadratic, but not much improvement after that.

#### k-fold Cross Validation

To perform k-fold cross validation, you can still use the `cv.glm()` function, define a value for the argument `k`.

```{r}
set.seed(17)
cv.error.10 <- rep(0,10)

for(i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}

cv.error.10


```

#### Bootstrap

**Estimating the accuracy of a statistic of interest**

Two steps: 1. Create a function to compute the statistic of interest ($\alpha$) 2. Use the `boot()` function to repeatedly generate bootstrapped data samples)

```{r}
alpha.fn <- function(data, index){
  X <- data$X[index]
  Y <- data$Y[index]
  (var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y))
}

```

The function returns an estimate for ($\alpha$) based on the specified index of the dataset.

```{r}
alpha.fn(Portfolio, 1:100)
```

Use `sample()` to randomly select 100 observations with replacement (i.e. creating a bootstrap dataset and then recalculating the estimate for ($\alpha$))

```{r}
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = TRUE))
```

To perform a bootstrap analysis (peformining the above step multiple times), we can use the `boot()` function:

```{r}
boot(
  data = Portfolio,
  statistic = alpha.fn,
  R = 1000
)
```

**Estimating the accuracy of a linear regression model** The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method.

First create a function to return the intercept and coefficient estimates for a linear regression model:

```{r}
#Define function (no brackets needed since it's only one line)
boot.fn <- function(data, index)
  coef(lm(mpg ~ horsepower, data = data, subset = index))

#
boot.fn(Auto, 1:392)
```

To create a bootstrap version, use the `sample()` function:

```{r}
set.seed(1)
boot.fn(Auto, sample(392, 392, TRUE))
```

Then, use the `boot()` function to compute the standard errors of 1000 bootstrap estimates of the intercept and slope.

```{r}
boot(Auto, boot.fn, 1000)
```

Compare to the estimates from the standard formula

```{r}
summary(lm(mpg ~ horsepower, data = Auto))$coef
```

The bootstrap method produced different Standard Errors, but they are often more accurate than the ones from summary(). Below is another comparison but with a quadratic fit.

```{r}
#Bootstrap method
boot.fn <- function(data, index)
  coef(
    lm(mpg ~ horsepower + I(horsepower^2),
       data = data, subset = index)
  )

set.seed(1)
boot(Auto, boot.fn, 1000)

#standard calculation
summary(
  lm(mpg ~ horsepower + I(horsepower^2), Auto)
)$coef
```

## Ch 6: Regularization

### Notes

Example: You have a linear model with p = 100 and want to find a subset model with the best test MSE. Which of the methods below is best to use?

**Subset Selection**

1.  Best Subset Selection

    1.  Tests all potential models with p = 1, p = 2, ... p =100. Total number of models to test is $2^p$

    2.  perform *p* cross validations to find the best test MSE

    3.  While this model is theoretically best, it is almost always unfeasible to calculate so many models

2.  Forward Stepwise Selection

    1.  Step 1: test out all possible p = 1 models (100), find the one that has the lowest test MSE or highest $R^2$

    2.  Step 2: test out all possible p = 2 models with $p_1$ being the predictor identified in step 1.

    3.  Steps 3 through *p*:In each step, use the identified predictors from the previous step and go through models with one additional predictor for remaining *p*s, adding one each time that has the lowest test MSE.

    4.  Total number of models to test is $(1+ p(p+1))/2$, far less than $2^p$

    5.  

3.  Backward Stepwise Selection

    1.  Start with full model of *p* predictors

    2.  Take away one variable at a time

    3.  Requires that *n* \> *p*

**Shrinkage Methods**

Reminder- least squares estimation uses RSS

1.  Ridge Regression
2.  The Lasso

**Principal Component Analysis (PCA)**

Same goal as other regularization methods above - reduce the number of features/parameters in a model.

Principal components are directions in the predictor space that capture the most variation

### Lab: Linear Models and Regularization Methods

#### Best Subset Selection

Use Best Subset Selection on `Hitters` data

```{r}
#Preview the data
names(Hitters)
dim(Hitters)
str(Hitters)

#Find the number of missing values for Salary
sum(is.na(Hitters$Salary))




```

The `na.omit()` function removes all of the rows that have missing values in any variable.

```{r}
Hitters <- na.omit(Hitters)

dim(Hitters)

sum(is.na(Hitters$Salary))
```

The regsubsets() function (part of the leaps library) performs best sub- set selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.

```{r}
library(leaps)

regfit.full <- regsubsets(Salary~., Hitters)

summary(regfit.full)
```

Returns up to 8-variable model by default, use nvmax argument for more:

```{r}
regfit.full <- regsubsets(Salary~., Hitters, nvmax = 19)

reg.summary <- summary(regfit.full)
```

The summary() function also returns R2, RSS, adjusted R2, Cp, and BIC. We can examine these to try to select the best overall model.

```{r}
names(reg.summary)

#view the R^2 statistic
reg.summary$rsq
```

Plotting RSS, adjusted R2, Cp, and BIC for all of the models at once will help us decide which model to select. Note the type = "l" option tells R to connect the plotted points with lines.

```{r}

par(mfrow = c(2,2))

plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")

which.max(reg.summary$adjr2)

points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
```

In a similar fashion we can plot the Cp and BIC statistics, and indicate the models with the smallest statistic using which.min().

```{r}
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
 points(10, reg.summary$cp[10], col = "red", cex = 2,
pch = 20)
 
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2,
pch = 20)


```

The regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, Cp, adjusted R2, or AIC. To find out more about this function, type ?plot.regsubsets.

```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```

The top row shows the variables of the optimal model for each statistic. To view the coefficients, determine the number of variables in the optimal model and use the coef() function:

```{r}
coef(regfit.full, 6)
```

#### Forward & Backward Stepwise Selection

```{r}

```

#### Ridge Regression

```{r}

```

#### The Lasso

```{r}

```

#### Principal Component Analysis

```{r}

```

## Ch 8: Tree Based Methods

### Notes

Decision trees can be applied to both regression and classification problems.

-   Internal nodes - where splits happen in the tree

-   Terminal nodes or "leaves" - the end points of the tree

Brute force approach

-   Split predictor space into all possible boxes, choose the ones that minimize RSS

-   This is not possible in practice

Therefore must use the "greedy" approach using one predictor at a time

1.  First compute RSS for all possible splits for all predictors

    1.  Choose one predictor and one split point combo that has minimum RSS

2.  Next step - create one more split of either of the two existing spaces.

When to stop splitting into more boxes?

-   Define a certain number of points in each box

-   If further splits is not improving RSS significantly

Pruning Trees (Cost Complexity Pruning)

-   The tuning parameter $\alpha$ controls a trade off between the subtree's complexity and its fit to the training data.

**Bagging (Bootstrap Aggregation)**

-   Based on the idea that averaging a set of observations reduces variance

-   Take many bootstrapped training sets and build models for all of them, average the resulting predictions

-   For classification, take the mode (i.e. the most frequently predicted class)

-   Problem: trees are correlated in the bagging process, certain predictors may dominate every tree

-   How many bootstrapped samples should there be? often *b* = 100

**Random Forests**

-   Decorellates the trees

-   Randomly selects a subset *m* of all predictors to consider for each split

-   What is the ideal value of *m?* Some common values are $p/2$ or $\sqrt p$

-   Variable importance - a way to see which predictors are most important, similar to coefficient values in linear regression

**Boosting**

-   A sequential process of small steps

-   Build a small tree, see what you learn and what you didn't learn (i.e. the residual)

-   Build each next tree focused on the remaining residual

-   Create summation of all trees for final prediction

-   Three tuning parameters

    -   $B$, the number of trees to build

    -   $\lambda$, the shrinkage parameter

    -   $d$ , the number of splits in each tree

### Lab

#### Fitting Classification Trees

```{r}
#Load package for decision trees
library(tree)
library(ISLR2)

#Load the Carseats sales dataset for analysis
attach(Carseats)

#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))

#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)

#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)

#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)

```

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]

tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)

#Calculate percentage of correct predictions
(104+50)/200
```
**Tree Pruning**
```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)

cv.carseats

par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")

prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)

tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)

(97+58)/200

```
The pruned tree is more interpretable and slightly more accurate for prediction. 

#### Fitting Regression Trees
```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)

plot(tree.boston)
text(tree.boston, pretty = 0)

#Perform cross validation on pruned trees
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```
The most complex tree is also the best in this case (7 branches). If we did need to prune the tree, we'd use this method (example of 5 branches):

```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)

#Use the unpruned tree (in this example) to make predictions
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(1,0)
mean((yhat - boston.test)^2)

```


#### Bagging & Random Forests
Bagging is a special case of random forest where *m* = *p*, so the `randomForest` package can be used for both techniques.

```{r}
library(randomForest)

set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, importance = TRUE)

bag.boston

```
The argument mtry = 12 indicates that all 12 predictors should be considered for each split of the tree—in other words, that bagging should be done. How well does this bagged model perform on the test set?

```{r}
#Use the Bagging method
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag - boston.test)^2)

bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```

Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classification trees. 

```{r}
#Using the Random Forest method
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)

```
Use the `importance()` function to view the importance of each variable
```{r}
importance(rf.boston)

#Plot the importance measures
varImpPlot(rf.boston)
```


#### Boosting

#### Bayesian Additive Regression Trees

## Ch 9: Support Vector Machines

### Notes

1.  Maximal Margin Classifier

    -   Separate observations based on a hyperplane (a p-1 dimensional plane, in the case of only 2 dimensions it's a line)

    -   Only can handle a small number of datasets that have a clean division of classes (the non-separable case)

    -   There can be multiple lines or hyperplanes that cleanly separate the classes. How do you choose one? With the maximal margin

    -   The MMC is the separating hyperplane that is farthest from the training observations (maximize *M*)

2.  Support Vector Classifier

    -   Use a "soft boundary" to be applied to more datasets - it's ok if some data points are misclassified or on the wrong side of the hyperplane

    -   Observations can be on the wrong side of the margin or on the wrong side of the hyperplane

    -   Slack Variables

        What happens if:

        -   \$ \epsilon\_i = 0 \$ same as MMC

        -   \$ 0 \< \epsilon\_i \< 0 \$

            On the right side of the hyperplane because it's positive, but inside the margin

        -   \$ \epsilon\_i \> 1 \$

            Violation of the hyperplane (on the wrong side, a misclassification)

        C tuning parameter

        -   Low C models are more flexible, prone to overfitting

    -   What are support vectors? the points on the wrong side of the margin (or on the margin)

3.  Support Vector Machines

    -   

4.  

### Lab
