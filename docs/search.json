[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains my notes and practice lab exercises for the course PA 397C Introduction to Machine Learning / Statistical Analysis and Learning.\nThe lab content on this site is from the textbook An Introduction to Statistical Learning with Applications in R, second edition\nCreated by: Michael Santulli\nDisclaimer: This site does not contain any information related to textbook exercises, homework, or any other course assignments. It’s contents is strictly limited to conceptual notes and the textbook labs using R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Machine Learning Notes",
    "section": "",
    "text": "The lab content on this site is from the textbook An Introduction to Statistical Learning with Applications in R, second edition"
  },
  {
    "objectID": "index.html#r-setup",
    "href": "index.html#r-setup",
    "title": "Intro to Machine Learning Notes",
    "section": "R Setup",
    "text": "R Setup\n\n#Load packages for chapter labs\nlibrary(\"ISLR2\") #Introduction to Statistical Learning datasets'\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "index.html#week-1-notes",
    "href": "index.html#week-1-notes",
    "title": "Intro to Machine Learning Notes",
    "section": "Week 1 Notes",
    "text": "Week 1 Notes\nTypes of machine learning:\n\nSupervised learning\n\nRegression models - quantitative/continous output\nClassification models - qualitative/discrete output\n\nUnsupervised learning\n\nClustering models - patterns from input data without specified output\n\n\nWhy study statistical learning?\n\nInference - how does a particular input drive an output variable\nPrediction - only the value of the outcome variable is of interest\n\nExample: “How much rainfall will California have in 2050?”?"
  },
  {
    "objectID": "index.html#chapter-2-statistical-learning-introduction",
    "href": "index.html#chapter-2-statistical-learning-introduction",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 2: Statistical Learning Introduction",
    "text": "Chapter 2: Statistical Learning Introduction\n\nNotes\nShared assumptions of linear and non-linear statistical models for estimating f:\n\nParametric vs. non-parametric\nFlexibility (complexity) vs. interpretability\n\nFlexible models are beneficial for prediction\nInterpretable models are beneficial for inference\n\nSupervised vs. unsupervised learning\n\nUnsupervised has no specified outcome variable\n\n\nHow to determine the best statistical model for a problem\n\nQuality of fit\nVariance/Bias tradeoff\n\nVariance: the amount by which \\(\\hat{f}\\) would change if estimated with a different training set\nBias: the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\nIn general, with more flexible methods, variance increases and bias decreases\n\n\nInference vs. Prediction\n\nSometimes both are of interest, but often only one is of primary interest. The goals of the analyst here are the primary driver in selecting a model\n\nHow do we estimate f?\n\nTraining data used to estimate f\nParametric approach\n\nStep 1: Specify an estimated functional form for f (i.e. a linear function)\nStep 2: Training data is used to estimate the parameters of the function\nDisadvantage of parametric methods: not well suited to estimate a function for a complex dataset\n\nNon-parametric approach\n\nAvoids the assumption of a particular functional form for f\nDisadvantage: they require very large data sets to make an accurate prediction of f\n\n\nWhy would we ever choose to use a more restrective method instead of a very flexible approach?\n\nInterpretability, inference, to avoid overfitting\n\nShould we always choose a more complex (flexible) approach when prediction is the objective?\n\nOnly if there is a very large dataset. With a small amount of data, more complexity is not always good\n\nMeasuring the Quality of Fit\n\nMean Squared Error (MSE)\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2\n\\]\nTraining MSE vs. Test MSE\nWhy does training MSE always decrease with added flexibility?\n\nWith enough flexibility you can get your model to perfectly fit the training data (but test MSE would be much worse)\n\nhighest quality of fit does not equal the best predictive model (overfitting)\n\n\n\nLab: Intro to R\nc() Create a vector of items\nlength() Returns the length of a vector\nls() Lists all objects such as data and functions saved in the environment\nrm() Remove an object from the environment\nmatrix(data = , nrow = , ncol =) Create a matrix\nsqrt() calculate the square root\nrnorm(n) Generates a vector of random normal variables of n sample size\ncor(x, y) Calculates the correlation between two sets of numbers\nset.seed() Used to set a consistent seed for a random number function\nmean() Mean\nvar() Variance\nsd() Standard Deviation\nplot() Basic plotting function\ncountour() Creates a countour plot to represent 3D data\n\nx &lt;- seq(1,10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\n\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\nimage() Produces a heatmap plot\npersp() Creates a 3D plot, arguments theta and phi control the viewing angles\n\nimage(x, y, fa)\n\n\n\npersp(x, y, fa)\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)\n\n\n\n\ndim() Dimension function returns the number of rows and columns of a matrix\nread.table() Import data\nwrite.table() Export data\nData Frame functions\ndata.frame() Create a data frame\nstr() Used to view a list of variables and first few observations in a data table\nsubset() Used to filter a data table\norder() Used to return the order of a vector, can sort a data table\nlist() Create a list"
  },
  {
    "objectID": "index.html#chapter-3-linear-regression",
    "href": "index.html#chapter-3-linear-regression",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 3: Linear Regression",
    "text": "Chapter 3: Linear Regression\n\nNotes\nInference problem example - which advertising strategy will lead to higher product sales next year?\nSimple linear regression is a a method for predicting a quantitative response Y on the basis of a single predictor variable X. A simple model uses the equation:\n\\[\nY \\approx \\beta_0 + \\beta_1x_1\n\\] Residual Sum of Squares (RSS) is the sum of differences between the observed vaues and predicted values\nLeast squares estimation method minimizes RSS to created an estimation line with the equation above. \\(\\beta_1\\) and \\(\\beta_0\\) are computable from the predictors and outcomes in the dataset\nStandard Errors for OLS estimates\nHypothesis testing steps - if the regression shows a positive or negative sloped line based on the sample, how can we be sure that it is not actually a flat line in the population?\n\nestimate parameters and standard errors\ncalculate t-statistic\nFind the corresponding p value\n\nWhen the t-statistic is large and the p value is low, we can reject the null hypothesis\n\n\nAccuracy of the model: how well does the model fit the data?\n\nRSE (Residual Standard Error)\n\nHow far on average are the actual outcomes from the prediction line?\n\n\\(R^2\\) statistic\n\nA proportional measure always between 0 and 1 that shows how much variation in the data is explained by the model\nCan \\(R^2\\) be negative? Technically yes if the model is very bad\n\nF-statistic\n\nNot about significance but about whether you can reject the null hypothesis for the whole model\n\n\nMultiple Linear Regression\nImportant questions in MLR:\n\nIs at least one of the predictors useful in predicting the response? Check with the F-statistic\nDo all the predictors help to explain Y, or is only a subset of the predictors useful? Will be covered in Ch. 6\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?\n\nConfidence intervals connect the sample variable to the population variable within a certain degree of confidence. A 95% confidence interval says that 95% of random samples will fall within the interval.\nPotential problems in MLR:\n\n\nLab: Regression in R\n\nSimple Linear Regression\nBoston dataset: The outome variable medv is median home value by census tract\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\n#view the first 10 observations of the dataset\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n#Create a regression equation \nattach(Boston)\nlm.fit &lt;- lm(medv ~ lstat)\n\n#View the regression results\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n#View details about the regression\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n#See what is stored in the lm.fit list\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n#Function to view the coefficients of lm.fit\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n#View the confidence interval\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n#Generate confidence intervals for given values of lstat\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\n#Generate prediction intervals for given values of lstat\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n#plot\nplot(lstat, medv)\n\n#Add the least squares line to the plot\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\n#Use 'col =' to change the color of the points\nplot(lstat, medv, col = \"red\")\n\n\n\n#Use 'pch =' to change the shape of the points\nplot(lstat, medv, pch = 20)\n\n\n\n#Define the point shape directly\nplot(lstat, medv, pch = \"+\")\n\n\n\n#Define the point shape with a number\nplot(1:20, 1:20, pch = 1:20)\n\n\n\npar(mfrow = c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\n\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\nMultiple Linear Regression\n\n#Run a regression with specified predictors\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n#Run a regression on all the predictor variables in the dataset\nlm.fit &lt;- lm(formula = medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n#Calculate variance inflation factors\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n#Run the regression all predictors except one (age) using the \"-\" sign\nlm.fit1 &lt;- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n#Another way to change the model using update()\nlm.fit1 &lt;- update(lm.fit, ~ . - age)\n\n\n\nInteraction terms\nThere are two ways to include interaction terms in the lm() funtion: \\(x_1:x_2\\) creates an interaction term between the two variables. \\(x_1 * x_2\\) creates an individual variable for each plus an interaction term.\n(\\(x_1 * x_2\\) is shorthand for \\(x_1 + x_2 + x_1:x_2\\))\n\n#Run a regression with a predictor variable\nsummary(lm(medv ~ lstat*age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nNon-linear transformations on predictors\nTo transform a variable in lm(), use I(). For example, to square a predictor you would use I(x^2).\nFor higher order variables, use the poly() function.\n\n#Run a regression with a squared predictor term\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n#Use anova() to see if the quadratic fit is better than the original linear fit\nlm.fit &lt;- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow = c(2,2))\nplot(lm.fit2)\n\n\n\n#Use the poly() function within lm() to generate a regression with higher order polynomials\nlm.fit5 &lt;- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n#Generate a regression with a log-transformed variable\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQualitative predictors\nlm() automatically generates dummy variables in a regression on a dataset with qualitative variables.\n\n#Preview the Carseats dataset with information about carseat sales\nstr(Carseats)\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n#A regression on Carseats data with all variables and some interaction terms\nlm.fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n#Use contrasts() to view the coding scheme for a qualitative variable\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\nWriting functions in R\n\n#Write a function to load relevant libraries\nLoadLibraries &lt;- function() {\n  library(ISLR2)\n  library(MASS)\n  print(\"The libraries have been loaded.\")\n}"
  },
  {
    "objectID": "index.html#chapter-4-classification",
    "href": "index.html#chapter-4-classification",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 4: Classification",
    "text": "Chapter 4: Classification\n\nNotes\nClassification techniques are used when the dependent variable Y is qualitative or categorical.\n\nLogistic regression\n\nEstimation process:\n\nCreate probability functions to estimate coefficients based on training data\nUse new probability equation to predict probabilities\nSet a boundary (i.e. .5) to classy to one category or the other\ninterpreting coefficients - change in log-odds, not change in probability\n\nNote: the coefficients estimated in logistic regression are the change in log-odds, however, these can be plugged back into an equation to calculate probability\n\nBayes Classifier\n\nCalculate probabilities of each classification outcome\nBayes classifier picks the largest probability to assign a class\nWhat kind of data is needed to calculate probabilities in step 1? You need a lot of it to make accurate calculations, and sufficient data is not often available. It can, however, be used with simulated data for examples.\n\nLinear Discriminant Analysis (LDA)\n\nBayes Theorem - used to flip conditional probabilities\nAssumption - the distributions of X are normal or Gaussian\nAssumption - both the outcome distributions have the same variance (not an assumption of QDA)\nData statistics needed to plug into the equation with two classes and one predictor: \\(\\sigma, \\pi_1, \\pi_2, \\mu_1, \\mu_2\\) where \\(\\sigma\\) is the shared variance, \\(\\pi_1\\) and \\(\\pi_2\\) are the prior probabilities of k = 1, 2, and \\(\\mu_1\\) and \\(\\mu_2\\) are the means\n\nQuadratic Discriminant Analysis (QDA)\nNaive Bayes\n\nComparison of methods\n\n\nLab: Classification Models in R\nSummary of functions in R: lm() Linear regression glm(...family = binomial) Logistic regression lda() Linear Discriminant Analysis (from the MASS package) qda() Quadratic Discriminant Analysis (from the MASS package) naiveBayes() Naive Bayes (from the e1071 package)\n\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\ncor(Smarket[,-9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n\nattach(Smarket)\nplot(Volume)\n\n\n\n\n\nLogistic Regression\nThe glm() function can be used for generalized linear models including logistic regression. Need to include the argument family = binomial to specify logistic regression as the method.\n\n#Construct a logistic regression model to predict the qualitative variable Direction\nglm.fits &lt;- glm(\n  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial\n)\n\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n#View the coefficients\ncoef(glm.fits)\n\n (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n      Volume \n 0.135440659 \n\n#Another way to view the coefficients\nsummary(glm.fits)$coef\n\n                Estimate Std. Error    z value  Pr(&gt;|z|)\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\n\n#Access just the p-values for the coefficients\nsummary(glm.fits)$coef[,4]\n\n(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 \n  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 \n     Volume \n  0.3924004 \n\n#Predict the probability that the market will go up or down\n#Set type = response to output probabilities of the form P(Y = 1|X)\nglm.probs &lt;- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\n#Check whether the probability is of the market going up or down (it is Up)\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n#Create a vector of predicted outcomes (up or down)\nglm.pred &lt;- rep(\"Down\", 1250)\nglm.pred[glm.probs &gt; .5] = \"Up\"\n\n#Generate a confusion matrix to see the correct and incorrect predictions\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n\nWhen building and testing models it is useful to split the data into training data and test data. This way you can see well your model will work when exposed to new data.\nStep 1: Train the model\n\n#Create vector of all observations before 2005\ntrain &lt;- (Year &lt; 2005)\n#Use vector to create a dataset of observations only from 2005,\n#Boolean vectors can be used to obtain a subset of the rows or columns of a matrix.\nSmarket.2005 &lt;- Smarket[!train,]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\n#Alternate method of filtering the data\nSmarket.2005b &lt;- subset(Smarket, Year &gt;= 2005)\n\n#Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.\nglm.fits &lt;- glm(\n  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n  data = Smarket, family = binomial, subset = train\n)\n\nStep 2: Test the model with new data\n\n#Predict direction of the stock market for only the data in 2005\nglm.probs &lt;- predict(glm.fits, Smarket.2005, type = \"response\")\n\nStep 3: Compare the predictions to the actual outcomes in the test data\n\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n#Test set success rate\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\n#Test set error rate\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe resulting error rate is not very good but not a surprising result for trying to predict future stock market conditions only based on the last few days. Below the analysis is repeated, but only keeping the predictors Lag 1 and Lag 2 which had the smallest P-values (i.e. higher significance)\n\nglm.fits &lt;- glm(\n  Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train\n)\nglm.probs &lt;- predict(glm.fits, Smarket.2005, type = \"response\")\nglm.pred &lt;- rep(\"Down\",250)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n\nPredictions can also be modified to use differenct coefficient values:\n\npredict(glm.fits,\n        newdata = \n          data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),\n        type = \"response\"\n)\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\nLinear Discriminant Analysis\n\nlibrary(MASS)\n#Set up a Linear Discriminant Analysis model\nlda.fit &lt;- lda(\n  Direction ~ Lag1 + Lag2, data = Smarket, subset = train\n)\n\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit)\n\n\n\nlda.pred &lt;- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nThe predict() function returns a list with three elements:\n\nclass LDA’s predictions about the movement of the market\nposterior a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class\nx contains the linear discriminants\n\n\nlda.class &lt;- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nIn this case, LDA produced similar results to Logistic Regression\n\nsum(lda.pred$posterior[,1] &gt;= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[,1] &lt; .5)\n\n[1] 182\n\nlda.pred$posterior[1:20,1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n#Adjust threshold above .5\nsum(lda.pred$posterior[,1] &gt; .9)\n\n[1] 0\n\n\n\n\nQuadratic Discriminant Analysis\n\n#Set up a Quadratic Discriminant Analysis model\nqda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nqda.class &lt;- predict(qda.fit, Smarket.2005)$class\n\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\n\n\nNaive Bayes\n\n#load library containing the naiveBayes() function\nlibrary(e1071)\n\n#Create Naive Bayes model\nnb.fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n#Use model for prediction\nnb.class &lt;- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds &lt;- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5,]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "index.html#chapter-5-resampling-methods",
    "href": "index.html#chapter-5-resampling-methods",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 5: Resampling Methods",
    "text": "Chapter 5: Resampling Methods\n\nNotes\nResampling methods: drawing samples from the training data to fit your model multiple times, comparing the goodness of fit across each sample.\n\nModel assessment - the process of evaluating a model’s performance\nModel selection - the process of the proper level of flexibility for a model\n\nCross-validation\nValidation Set approach\n\nIf a test dataset is not available, you can split the training data into two sets:\n\ntraining set\nvalidation set\n\nFit the model using the training set, and then test it on the validation set.\n\nYou can also do this multiple times by using different random selections for the training and validation sets\n\nCreating a validation set is a useful and simple approach but has two drawbacks:\n\nThe validation estimate of the test error can vary substantially depending on how the data is split up\nOnly a subset of observations are used to train the model, since some must be withheld for the test/validation set.\n\n\nLeave One Out Cross Validation (LOOCV)\n\nSimilar to the validation set approach, but instead of splitting the observations in half, simply split one observation off for validation and use the rest (n-1) for the training set\nRepeat the procedure n times, calculating the MSE each time\nLOOCV estimate is the mean of all the MSEs\n\nCentral limit theorem does not apply since there are correlations between your samples\n\nDownside - has to be fit n times which can be time consuming or expensive\n\nA special rule allows a shortcut for linear or polynomial regression, needing only one fit\n\n\nk-Fold Cross Validation\n\nDivide the observations into k folds of equal size, using one for validation and the rest for training\nRepeat the procedure k times using a different fold as the validation set, recording MSE\nTake the average of all the MSEs\nTypically performed using k = 5 or k = 10\n\nCross validation with classification\n\nSame as above, but instead of using MSE to quantify test error, use the number of misclassified observations\n\n\nBootstrap\n\n\nLab: Cross Validation and the Bootstrap\n\nValidation Set Approach\n\nlibrary(ISLR2)\nset.seed(1)\n\n#Split data in two for training and testing\ntrain &lt;- sample(392, 196)\n\nlm.fit &lt;- lm(mpg ~ horsepower, data = Auto, subset = train)\n\n#Calculate the test MSE\nattach(Auto)\n\nThe following object is masked from package:lubridate:\n\n    origin\n\n\nThe following object is masked from package:ggplot2:\n\n    mpg\n\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 23.26601\n\n#Fit a quadratic and cubic model with the poly() function\nlm.fit2 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\nmean((mpg- predict(lm.fit2, Auto))[-train]^2)\n\n[1] 18.71646\n\nlm.fit3 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\nmean((mpg- predict(lm.fit3, Auto))[-train]^2)\n\n[1] 18.79401\n\n\nAnalysis of the MSe for each model indicates that a quadratic function is an improvement in the linear model, but a cubic model does not offer any significant additional improvement.\n\n\nLeave One Out Cross Validation\nYou can perform LOOCV using cv.glm() For linear regression, instead of using lm(), use the glm() function but leave the family argument blank.\n\n#boot package contains cv.glm()\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\nglm.fit &lt;- glm(mpg ~ horsepower, data = Auto)\ncv.err &lt;- cv.glm(Auto, glm.fit)\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\n$delta contains the CV results The first numbers is the standard CV estimate; the second is a bias-corrected version. In this case they are nearly the same.\nTo repeat the process of cross validation over multiple polynomial fits, use a for loop:\n\ncv.error &lt;- rep(0,10)\n\nfor (i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] &lt;- cv.glm(Auto, glm.fit)$delta[1]\n}\n\n#Display the test MSE for each all polynomial models\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093\n\n\nThere is a decrease in test MSE from linear to quadratic, but not much improvement after that.\n\n\nk-fold Cross Validation\nTo perform k-fold cross validation, you can still use the cv.glm() function, define a value for the argument k.\n\nset.seed(17)\ncv.error.10 &lt;- rep(0,10)\n\nfor(i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] &lt;- cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\n\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\n\nBootstrap\nEstimating the accuracy of a statistic of interest\nTwo steps: 1. Create a function to compute the statistic of interest (\\(\\alpha\\)) 2. Use the boot() function to repeatedly generate bootstrapped data samples)\n\nalpha.fn &lt;- function(data, index){\n  X &lt;- data$X[index]\n  Y &lt;- data$Y[index]\n  (var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y))\n}\n\nThe function returns an estimate for (\\(\\alpha\\)) based on the specified index of the dataset.\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.5758321\n\n\nUse sample() to randomly select 100 observations with replacement (i.e. creating a bootstrap dataset and then recalculating the estimate for (\\(\\alpha\\)))\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = TRUE))\n\n[1] 0.5385326\n\n\nTo perform a bootstrap analysis (peformining the above step multiple times), we can use the boot() function:\n\nboot(\n  data = Portfolio,\n  statistic = alpha.fn,\n  R = 1000\n)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0007959475  0.08969074\n\n\nEstimating the accuracy of a linear regression model The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method.\nFirst create a function to return the intercept and coefficient estimates for a linear regression model:\n\n#Define function (no brackets needed since it's only one line)\nboot.fn &lt;- function(data, index)\n  coef(lm(mpg ~ horsepower, data = data, subset = index))\n\n#\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nTo create a bootstrap version, use the sample() function:\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, TRUE))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\n\nThen, use the boot() function to compute the standard errors of 1000 bootstrap estimates of the intercept and slope.\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0549915227 0.841925746\nt2* -0.1578447 -0.0006210818 0.007348956\n\n\nCompare to the estimates from the standard formula\n\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nThe bootstrap method produced different Standard Errors, but they are often more accurate than the ones from summary(). Below is another comparison but with a quadratic fit.\n\n#Bootstrap method\nboot.fn &lt;- function(data, index)\n  coef(\n    lm(mpg ~ horsepower + I(horsepower^2),\n       data = data, subset = index)\n  )\n\nset.seed(1)\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\n#standard calculation\nsummary(\n  lm(mpg ~ horsepower + I(horsepower^2), Auto)\n)$coef\n\n                    Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21"
  },
  {
    "objectID": "index.html#ch-6-regularization",
    "href": "index.html#ch-6-regularization",
    "title": "Intro to Machine Learning Notes",
    "section": "Ch 6: Regularization",
    "text": "Ch 6: Regularization\n\nNotes\nExample: You have a linear model with p = 100 and want to find a subset model with the best test MSE. Which of the methods below is best to use?\nSubset Selection\n\nBest Subset Selection\n\nTests all potential models with p = 1, p = 2, … p =100. Total number of models to test is \\(2^p\\)\nperform p cross validations to find the best test MSE\nWhile this model is theoretically best, it is almost always unfeasible to calculate so many models\n\nForward Stepwise Selection\n\nStep 1: test out all possible p = 1 models (100), find the one that has the lowest test MSE or highest \\(R^2\\)\nStep 2: test out all possible p = 2 models with \\(p_1\\) being the predictor identified in step 1.\nSteps 3 through p:In each step, use the identified predictors from the previous step and go through models with one additional predictor for remaining ps, adding one each time that has the lowest test MSE.\nTotal number of models to test is \\((1+ p(p+1))/2\\), far less than \\(2^p\\)\n\n\nBackward Stepwise Selection\n\nStart with full model of p predictors\nTake away one variable at a time\nRequires that n &gt; p\n\n\nShrinkage Methods\nReminder- least squares estimation uses RSS\n\nRidge Regression\nThe Lasso\n\nPrincipal Component Analysis (PCA)\nSame goal as other regularization methods above - reduce the number of features/parameters in a model.\nPrincipal components are directions in the predictor space that capture the most variation\n\n\nLab: Linear Models and Regularization Methods\n\nBest Subset Selection\nUse Best Subset Selection on Hitters data\n\n#Preview the data\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nstr(Hitters)\n\n'data.frame':   322 obs. of  20 variables:\n $ AtBat    : int  293 315 479 496 321 594 185 298 323 401 ...\n $ Hits     : int  66 81 130 141 87 169 37 73 81 92 ...\n $ HmRun    : int  1 7 18 20 10 4 1 0 6 17 ...\n $ Runs     : int  30 24 66 65 39 74 23 24 26 49 ...\n $ RBI      : int  29 38 72 78 42 51 8 24 32 66 ...\n $ Walks    : int  14 39 76 37 30 35 21 7 8 65 ...\n $ Years    : int  1 14 3 11 2 11 2 3 2 13 ...\n $ CAtBat   : int  293 3449 1624 5628 396 4408 214 509 341 5206 ...\n $ CHits    : int  66 835 457 1575 101 1133 42 108 86 1332 ...\n $ CHmRun   : int  1 69 63 225 12 19 1 0 6 253 ...\n $ CRuns    : int  30 321 224 828 48 501 30 41 32 784 ...\n $ CRBI     : int  29 414 266 838 46 336 9 37 34 890 ...\n $ CWalks   : int  14 375 263 354 33 194 24 12 8 866 ...\n $ League   : Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 2 1 2 1 ...\n $ Division : Factor w/ 2 levels \"E\",\"W\": 1 2 2 1 1 2 1 2 2 1 ...\n $ PutOuts  : int  446 632 880 200 805 282 76 121 143 0 ...\n $ Assists  : int  33 43 82 11 40 421 127 283 290 0 ...\n $ Errors   : int  20 10 14 3 4 25 7 9 19 0 ...\n $ Salary   : num  NA 475 480 500 91.5 750 70 100 75 1100 ...\n $ NewLeague: Factor w/ 2 levels \"A\",\"N\": 1 2 1 2 2 1 1 1 2 1 ...\n\n#Find the number of missing values for Salary\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\nThe na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters &lt;- na.omit(Hitters)\n\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best sub- set selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nlibrary(leaps)\n\nregfit.full &lt;- regsubsets(Salary~., Hitters)\n\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nReturns up to 8-variable model by default, use nvmax argument for more:\n\nregfit.full &lt;- regsubsets(Salary~., Hitters, nvmax = 19)\n\nreg.summary &lt;- summary(regfit.full)\n\nThe summary() function also returns R2, RSS, adjusted R2, Cp, and BIC. We can examine these to try to select the best overall model.\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n#view the R^2 statistic\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted R2, Cp, and BIC for all of the models at once will help us decide which model to select. Note the type = “l” option tells R to connect the plotted points with lines.\n\npar(mfrow = c(2,2))\n\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\n\nwhich.max(reg.summary$adjr2)\n\n[1] 11\n\npoints(11, reg.summary$adjr2[11], col = \"red\", cex = 2, pch = 20)\n\n\n\n\nIn a similar fashion we can plot the Cp and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\nwhich.min(reg.summary$cp)\n\n[1] 10\n\n points(10, reg.summary$cp[10], col = \"red\", cex = 2,\npch = 20)\n\n\n\nwhich.min(reg.summary$bic)\n\n[1] 6\n\nplot(reg.summary$bic, xlab = \"Number of Variables\",\nylab = \"BIC\", type = \"l\")\npoints(6, reg.summary$bic[6], col = \"red\", cex = 2,\npch = 20)\n\n\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, Cp, adjusted R2, or AIC. To find out more about this function, type ?plot.regsubsets.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\nThe top row shows the variables of the optimal model for each statistic. To view the coefficients, determine the number of variables in the optimal model and use the coef() function:\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\nForward & Backward Stepwise Selection\n\n\nRidge Regression\n\n\nThe Lasso\n\n\nPrincipal Component Analysis"
  },
  {
    "objectID": "index.html#ch-8-tree-based-methods",
    "href": "index.html#ch-8-tree-based-methods",
    "title": "Intro to Machine Learning Notes",
    "section": "Ch 8: Tree Based Methods",
    "text": "Ch 8: Tree Based Methods\n\nNotes\nDecision trees can be applied to both regression and classification problems.\n\nInternal nodes - where splits happen in the tree\nTerminal nodes or “leaves” - the end points of the tree\n\nBrute force approach\n\nSplit predictor space into all possible boxes, choose the ones that minimize RSS\nThis is not possible in practice\n\nTherefore must use the “greedy” approach using one predictor at a time\n\nFirst compute RSS for all possible splits for all predictors\n\nChoose one predictor and one split point combo that has minimum RSS\n\nNext step - create one more split of either of the two existing spaces.\n\nWhen to stop splitting into more boxes?\n\nDefine a certain number of points in each box\nIf further splits is not improving RSS significantly\n\nPruning Trees (Cost Complexity Pruning)\n\nThe tuning parameter \\(\\alpha\\) controls a trade off between the subtree’s complexity and its fit to the training data.\n\nBagging (Bootstrap Aggregation)\n\nBased on the idea that averaging a set of observations reduces variance\nTake many bootstrapped training sets and build models for all of them, average the resulting predictions\nFor classification, take the mode (i.e. the most frequently predicted class)\nProblem: trees are correlated in the bagging process, certain predictors may dominate every tree\nHow many bootstrapped samples should there be? often b = 100\n\nRandom Forests\n\nDecorellates the trees\nRandomly selects a subset m of all predictors to consider for each split\nWhat is the ideal value of m? Some common values are \\(p/2\\) or \\(\\sqrt p\\)\nVariable importance - a way to see which predictors are most important, similar to coefficient values in linear regression\n\nBoosting\n\nA sequential process of small steps\nBuild a small tree, see what you learn and what you didn’t learn (i.e. the residual)\nBuild each next tree focused on the remaining residual\nCreate summation of all trees for final prediction\nThree tuning parameters\n\n\\(B\\), the number of trees to build\n\\(\\lambda\\), the shrinkage parameter\n\\(d\\) , the number of splits in each tree\n\n\n\n\nLab\n\nFitting Classification Trees\n\n#Load package for decision trees\nlibrary(tree)\nlibrary(ISLR2)\n\n#Load the Carseats sales dataset for analysis\nattach(Carseats)\n\nThe following objects are masked from Carseats (pos = 9):\n\n    Advertising, Age, CompPrice, Education, Income, Population, Price,\n    Sales, ShelveLoc, Urban, US\n\n#Create new variable to classify sales as High or low\nHigh &lt;- factor(ifelse(Sales &lt;= 8, \"No\", \"Yes\"))\n\n#Combine High variable with Carseats data\nCarseats &lt;- data.frame(Carseats, High)\n\n#Create classification tree model to predict High with all variables except Sales\ntree.carseats &lt;- tree(High ~ . - Sales, Carseats)\n\n#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.\nsummary(tree.carseats)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n\n\n\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0)\n\n\n\n\n\nset.seed(2)\ntrain &lt;- sample(1:nrow(Carseats), 200)\nCarseats.test &lt;- Carseats[-train,]\nHigh.test &lt;- High[-train]\n\ntree.carseats &lt;- tree(High ~ . - Sales, Carseats, subset = train)\ntree.pred &lt;- predict(tree.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\n#Calculate percentage of correct predictions\n(104+50)/200\n\n[1] 0.77\n\n\nTree Pruning\n\nset.seed(7)\ncv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.carseats\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n\n\n\nprune.carseats &lt;- prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n\ntree.pred &lt;- predict(prune.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred No Yes\n      No  97  25\n      Yes 20  58\n\n(97+58)/200\n\n[1] 0.775\n\n\n\n\n\nThe pruned tree is more interpretable and slightly more accurate for prediction.\n\n\nFitting Regression Trees\n\nset.seed(1)\ntrain &lt;- sample(1:nrow(Boston), nrow(Boston)/2)\ntree.boston &lt;- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n\n\n\n#Perform cross validation on pruned trees\ncv.boston &lt;- cv.tree(tree.boston)\nplot(cv.boston$size, cv.boston$dev, type = \"b\")\n\n\n\n\nThe most complex tree is also the best in this case (7 branches). If we did need to prune the tree, we’d use this method (example of 5 branches):\n\nprune.boston &lt;- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n\n\n\n#Use the unpruned tree (in this example) to make predictions\nyhat &lt;- predict(tree.boston, newdata = Boston[-train, ])\nboston.test &lt;- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(1,0)\n\n\n\nmean((yhat - boston.test)^2)\n\n[1] 35.28688\n\n\n\n\nBagging & Random Forests\nBagging is a special case of random forest where m = p, so the randomForest package can be used for both techniques.\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nset.seed(1)\nbag.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, importance = TRUE)\n\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.10176\n                    % Var explained: 85.56\n\n\nThe argument mtry = 12 indicates that all 12 predictors should be considered for each split of the tree—in other words, that bagging should be done. How well does this bagged model perform on the test set?\n\n#Use the Bagging method\nyhat.bag &lt;- predict(bag.boston, newdata = Boston[-train, ])\nplot(yhat.bag, boston.test)\nabline(0,1)\n\n\n\nmean((yhat.bag - boston.test)^2)\n\n[1] 23.38773\n\nbag.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)\nyhat.bag &lt;- predict(bag.boston, newdata = Boston[-train, ])\nmean((yhat.bag - boston.test)^2)\n\n[1] 25.19144\n\n\nGrowing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classification trees.\n\n#Using the Random Forest method\nset.seed(1)\nrf.boston &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)\nyhat.rf &lt;- predict(rf.boston, newdata = Boston[-train, ])\nmean((yhat.rf - boston.test)^2)\n\n[1] 19.62021\n\n\nUse the importance() function to view the importance of each variable\n\nimportance(rf.boston)\n\n          %IncMSE IncNodePurity\ncrim    16.697017    1076.08786\nzn       3.625784      88.35342\nindus    4.968621     609.53356\nchas     1.061432      52.21793\nnox     13.518179     709.87339\nrm      32.343305    7857.65451\nage     13.272498     612.21424\ndis      9.032477     714.94674\nrad      2.878434      95.80598\ntax      9.118801     364.92479\nptratio  8.467062     823.93341\nblack    7.579482     275.62272\nlstat   27.129817    6027.63740\n\n#Plot the importance measures\nvarImpPlot(rf.boston)\n\n\n\n\n\n\nBoosting\n\n\nBayesian Additive Regression Trees"
  },
  {
    "objectID": "index.html#ch-9-support-vector-machines",
    "href": "index.html#ch-9-support-vector-machines",
    "title": "Intro to Machine Learning Notes",
    "section": "Ch 9: Support Vector Machines",
    "text": "Ch 9: Support Vector Machines\n\nNotes\n\nMaximal Margin Classifier\n\nSeparate observations based on a hyperplane (a p-1 dimensional plane, in the case of only 2 dimensions it’s a line)\nOnly can handle a small number of datasets that have a clean division of classes (the non-separable case)\nThere can be multiple lines or hyperplanes that cleanly separate the classes. How do you choose one? With the maximal margin\nThe MMC is the separating hyperplane that is farthest from the training observations (maximize M)\n\nSupport Vector Classifier\n\nUse a “soft boundary” to be applied to more datasets - it’s ok if some data points are misclassified or on the wrong side of the hyperplane\nObservations can be on the wrong side of the margin or on the wrong side of the hyperplane\nSlack Variables\nWhat happens if:\n\n$ _i = 0 $ same as MMC\n$ 0 &lt; _i &lt; 0 $\nOn the right side of the hyperplane because it’s positive, but inside the margin\n$ _i &gt; 1 $\nViolation of the hyperplane (on the wrong side, a misclassification)\n\nC tuning parameter\n\nLow C models are more flexible, prone to overfitting\n\nWhat are support vectors? the points on the wrong side of the margin (or on the margin)\n\nSupport Vector Machines\n\n\n\n\n\n\n\nLab\n\nSupport Vector Classifier\n\nlibrary(e1071)\n\n#Create dataset\nset.seed(1)\nx &lt;- matrix(rnorm(20 * 2), ncol = 2)\ny &lt;- c(rep(-1,10),rep(1,10))\nx[y == 1, ] &lt;- x[y == 1]+1\nplot(x, col = (3-y))\n\n\n\n#Response must be a factor\ndat = data.frame(x = x, y = as.factor(y))\n#Use scale = TRUE to scale each factor to have mean of 0 and sd of 1\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", cost = 10, scale = FALSE)\n\nplot(svmfit, dat)\n\n\n\n#Print the support vectors\nsvmfit$index\n\n[1]  1  2  5  7 14 16 17\n\nsummary(svmfit)\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10, scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  7\n\n ( 4 3 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\n\n#Try using a smaller value for cost:\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", cost = 0.1, scale = FALSE)\n\nplot(svmfit, dat)\n\n\n\nsvmfit$index\n\n [1]  1  2  3  4  5  7  9 10 12 13 14 15 16 17 18 20\n\n\nThe tune() function performs 10-fold cross validation:\n\nset.seed(1)\ntune.out &lt;- tune(svm, y~., data = dat, kernel = \"linear\", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  0.1\n\n- best performance: 0.05 \n\n- Detailed performance results:\n   cost error dispersion\n1 1e-03  0.55  0.4377975\n2 1e-02  0.55  0.4377975\n3 1e-01  0.05  0.1581139\n4 1e+00  0.15  0.2415229\n5 5e+00  0.15  0.2415229\n6 1e+01  0.15  0.2415229\n7 1e+02  0.15  0.2415229\n\nbestmod &lt;- tune.out$best.model\nsummary(bestmod)\n\n\nCall:\nbest.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n    0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.1 \n\nNumber of Support Vectors:  16\n\n ( 8 8 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\n\n#Generate test dataset\nset.seed(1)\nxtest &lt;- matrix(rnorm(20 * 2), ncol = 2)\nytest &lt;- sample(c(-1, 1), 20, rep = TRUE)\nxtest[ytest == 1, ] &lt;- xtest[ytest == 1, ] + 1\ntestdat &lt;- data.frame(x = xtest, y = as.factor(ytest))\n\nypred &lt;- predict(bestmod, testdat)\ntable(predict = ypred, truth = testdat$y)\n\n       truth\npredict -1 1\n     -1  9 3\n     1   2 6\n\n#make \nx[y == 1, ] &lt;- x[y == 1, ] + 0.5\nplot(x, col = (y + 5) / 2, pch = 19)\n\n\n\ndat &lt;- data.frame(x = x, y = as.factor(y))\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", cost = 1e5)\nsummary(svmfit)\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 1e+05)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1e+05 \n\nNumber of Support Vectors:  3\n\n ( 1 2 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\nplot(svmfit, dat)\n\n\n\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", cost = 1)\nsummary(svmfit)\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  7\n\n ( 4 3 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\nplot(svmfit, dat)\n\n\n\n\n\n\nSupport Vector Machine\nTo fit an SVM with a polynomial kernel we use kernel = “polynomial”, and to fit an SVM with a radial kernel we use kernel = “radial”. In the former case we also use the degree argument to specify a degree for the polynomial kernel (this is d in (9.22)), and in the latter case we use gamma to specify a value of γ for the radial basis kernel (9.24).\n\n#generate data\nset.seed(1)\nx &lt;- matrix(rnorm(200 * 2), ncol = 2)\nx[1:100, ] &lt;- x[1:100, ] + 2\nx[101:150, ] &lt;- x[101:150, ] - 2\ny &lt;- c(rep(1, 150), rep(2, 50))\ndat &lt;- data.frame(x = x, y = as.factor(y))\n\nplot(x, col = y)\n\n\n\n#split the data\ntrain &lt;- sample(200,100)\nsvmfit &lt;- svmfit &lt;- svm(y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, cost = 1)\nplot(svmfit, dat[train, ])\n\n\n\nsummary(svmfit)\n\n\nCall:\nsvm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, \n    cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  31\n\n ( 16 15 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n\n\nIncreasing the value of cost can reduce training error, but at the risk of overfitting.\nThe tune function can perform cross validation for both gamma and cost on an SVM with a radial kernel:\n\nset.seed(1)\ntune.out &lt;- tune(svm, y~., data = dat[train, ],\n                 kernel = \"radial\",\n                 ranges = list(\n                   cost = c(0.1, 1, 10, 100, 1000),\n                   gamma = c(0.5, 1, 2, 3, 4)\n                 )\n)\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.07 \n\n- Detailed performance results:\n    cost gamma error dispersion\n1  1e-01   0.5  0.26 0.15776213\n2  1e+00   0.5  0.07 0.08232726\n3  1e+01   0.5  0.07 0.08232726\n4  1e+02   0.5  0.14 0.15055453\n5  1e+03   0.5  0.11 0.07378648\n6  1e-01   1.0  0.22 0.16193277\n7  1e+00   1.0  0.07 0.08232726\n8  1e+01   1.0  0.09 0.07378648\n9  1e+02   1.0  0.12 0.12292726\n10 1e+03   1.0  0.11 0.11005049\n11 1e-01   2.0  0.27 0.15670212\n12 1e+00   2.0  0.07 0.08232726\n13 1e+01   2.0  0.11 0.07378648\n14 1e+02   2.0  0.12 0.13165612\n15 1e+03   2.0  0.16 0.13498971\n16 1e-01   3.0  0.27 0.15670212\n17 1e+00   3.0  0.07 0.08232726\n18 1e+01   3.0  0.08 0.07888106\n19 1e+02   3.0  0.13 0.14181365\n20 1e+03   3.0  0.15 0.13540064\n21 1e-01   4.0  0.27 0.15670212\n22 1e+00   4.0  0.07 0.08232726\n23 1e+01   4.0  0.09 0.07378648\n24 1e+02   4.0  0.13 0.14181365\n25 1e+03   4.0  0.15 0.13540064\n\n#Predict using the best model\ntable(\n  true = dat[-train, \"y\"],\n  pred = predict(\n    tune.out$best.model, newdata = dat[-train,]\n  )\n)\n\n    pred\ntrue  1  2\n   1 67 10\n   2  2 21\n\n\n\n\nROC Curves\n\n\nSVM with Multiple Classes"
  }
]