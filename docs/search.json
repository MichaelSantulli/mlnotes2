[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains my notes for the course PA 397C Introduction to Machine Learning / Statistical Analysis and Learning.\nIt has also served the purpose of expanding my knowledge of using git and Quarto for publishing documents on the web.\nCreated by: Michael Santulli\nDisclaimer: This site does not contain any information related to textbook exercises, homework, or any other course assignments. It’s contents is strictly limited to conceptual notes and the textbook labs using R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Machine Learning Notes",
    "section": "",
    "text": "#Load packages for chapter labs\nlibrary(\"ISLR2\") #Introduction to Statistical Learning datasets'\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "index.html#r-setup",
    "href": "index.html#r-setup",
    "title": "Intro to Machine Learning Notes",
    "section": "",
    "text": "#Load packages for chapter labs\nlibrary(\"ISLR2\") #Introduction to Statistical Learning datasets'\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "index.html#week-1-notes",
    "href": "index.html#week-1-notes",
    "title": "Intro to Machine Learning Notes",
    "section": "Week 1 Notes",
    "text": "Week 1 Notes\nTypes of machine learning:\n\nSupervised learning\n\nRegression models - quantitative/continous output\nClassification models - qualitative/discrete output\n\nUnsupervised learning\n\nClustering models - patterns from input data without specified output\n\n\nWhy study statistical learning?\n\nInference - how does a particular input drive an output variable\nPrediction - only the value of the outcome variable is of interest\n\nExample: “How much rainfall will California have in 2050?”?"
  },
  {
    "objectID": "index.html#chapter-2-statistical-learning-introduction",
    "href": "index.html#chapter-2-statistical-learning-introduction",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 2: Statistical Learning Introduction",
    "text": "Chapter 2: Statistical Learning Introduction\n\nNotes\nShared assumptions of linear and non-linear statistical models for estimating f:\n\nParametric vs. non-parametric\nFlexibility (complexity) vs. interpretability\n\nFlexible models are beneficial for prediction\nInterpretable models are beneficial for inference\n\nSupervised vs. unsupervised learning\n\nUnsupervised has no specified outcome variable\n\n\nHow to determine the best statistical model for a problem\n\nQuality of fit\nVariance/Bias tradeoff\n\nVariance: the amount by which \\(\\hat{f}\\) would change if estimated with a different training set\nBias: the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\nIn general, with more flexible methods, variance increases and bias decreases\n\n\nInference vs. Prediction\n\nSometimes both are of interest, but often only one is of primary interest. The goals of the analyst here are the primary driver in selecting a model\n\nHow do we estimate f?\n\nTraining data used to estimate f\nParametric approach\n\nStep 1: Specify an estimated functional form for f (i.e. a linear function)\nStep 2: Training data is used to estimate the parameters of the function\nDisadvantage of parametric methods: not well suited to estimate a function for a complex dataset\n\nNon-parametric approach\n\nAvoids the assumption of a particular functional form for f\nDisadvantage: they require very large data sets to make an accurate prediction of f\n\n\nWhy would we ever choose to use a more restrective method instead of a very flexible approach?\n\nInterpretability, inference, to avoid overfitting\n\nShould we always choose a more complex (flexible) approach when prediction is the objective?\n\nOnly if there is a very large dataset. With a small amount of data, more complexity is not always good\n\nMeasuring the Quality of Fit\n\nMean Squared Error (MSE)\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2\n\\]\nTraining MSE vs. Test MSE\nWhy does training MSE always decrease with added flexibility?\n\nWith enough flexibility you can get your model to perfectly fit the training data (but test MSE would be much worse)\n\nhighest quality of fit does not equal the best predictive model (overfitting)\n\n\n\nLab: Intro to R\nc() Create a vector of items\nlength() Returns the length of a vector\nls() Lists all objects such as data and functions saved in the environment\nrm() Remove an object from the environment\nmatrix(data = , nrow = , ncol =) Create a matrix\nsqrt() calculate the square root\nrnorm(n) Generates a vector of random normal variables of n sample size\ncor(x, y) Calculates the correlation between two sets of numbers\nset.seed() Used to set a consistent seed for a random number function\nmean() Mean\nvar() Variance\nsd() Standard Deviation\nplot() Basic plotting function\ncountour() Creates a countour plot to represent 3D data\n\nx &lt;- seq(1,10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\n\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\nimage() Produces a heatmap plot\npersp() Creates a 3D plot, arguments theta and phi control the viewing angles\n\nimage(x, y, fa)\n\n\n\npersp(x, y, fa)\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)\n\n\n\n\ndim() Dimension function returns the number of rows and columns of a matrix\nread.table() Import data\nwrite.table() Export data\nData Frame functions\ndata.frame() Create a data frame\nstr() Used to view a list of variables and first few observations in a data table\nsubset() Used to filter a data table\norder() Used to return the order of a vector, can sort a data table\nlist() Create a list"
  },
  {
    "objectID": "index.html#chapter-3-linear-regression",
    "href": "index.html#chapter-3-linear-regression",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 3: Linear Regression",
    "text": "Chapter 3: Linear Regression\n\nNotes\nInference problem example - which advertising strategy will lead to higher product sales next year?\nSimple linear regression is a a method for predicting a quantitative response Y on the basis of a single predictor variable X. A simple model uses the equation:\n\\[\nY \\approx \\beta_0 + \\beta_1x_1\n\\] Residual Sum of Squares (RSS) is the sum of differences between the observed vaues and predicted values\nLeast squares estimation method minimizes RSS to created an estimation line with the equation above. \\(\\beta_1\\) and \\(\\beta_0\\) are computable from the predictors and outcomes in the dataset\nStandard Errors for OLS estimates\nHypothesis testing steps - if the regression shows a positive or negative sloped line based on the sample, how can we be sure that it is not actually a flat line in the population?\n\nestimate parameters and standard errors\ncalculate t-statistic\nFind the corresponding p value\n\nWhen the t-statistic is large and the p value is low, we can reject the null hypothesis\n\n\nAccuracy of the model: how well does the model fit the data?\n\nRSE (Residual Standard Error)\n\nHow far on average are the actual outcomes from the prediction line?\n\n\\(R^2\\) statistic\n\nA proportional measure always between 0 and 1 that shows how much variation in the data is explained by the model\nCan \\(R^2\\) be negative? Technically yes if the model is very bad\n\nF-statistic\n\nNot about significance but about whether you can reject the null hypothesis for the whole model\n\n\nMultiple Linear Regression\nImportant questions in MLR:\n\nIs at least one of the predictors useful in predicting the response? Check with the F-statistic\nDo all the predictors help to explain Y, or is only a subset of the predictors useful? Will be covered in Ch. 6\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?\n\nConfidence intervals connect the sample variable to the population variable within a certain degree of confidence. A 95% confidence interval says that 95% of random samples will fall within the interval.\nPotential problems in MLR:\n\n\nLab: Regression in R\n\nSimple Linear Regression\nBoston dataset: The outome variable medv is median home value by census tract\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\n#view the first 10 observations of the dataset\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n#Create a regression equation \nattach(Boston)\nlm.fit &lt;- lm(medv ~ lstat)\n\n#View the regression results\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n#View details about the regression\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n#See what is stored in the lm.fit list\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n#Function to view the coefficients of lm.fit\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n#View the confidence interval\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n#Generate confidence intervals for given values of lstat\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\n#Generate prediction intervals for given values of lstat\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n#plot\nplot(lstat, medv)\n\n#Add the least squares line to the plot\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\n#Use 'col =' to change the color of the points\nplot(lstat, medv, col = \"red\")\n\n\n\n#Use 'pch =' to change the shape of the points\nplot(lstat, medv, pch = 20)\n\n\n\n#Define the point shape directly\nplot(lstat, medv, pch = \"+\")\n\n\n\n#Define the point shape with a number\nplot(1:20, 1:20, pch = 1:20)\n\n\n\npar(mfrow = c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\n\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\nMultiple Linear Regression\n\n#Run a regression with specified predictors\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n#Run a regression on all the predictor variables in the dataset\nlm.fit &lt;- lm(formula = medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n#Calculate variance inflation factors\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n#Run the regression all predictors except one (age) using the \"-\" sign\nlm.fit1 &lt;- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n#Another way to change the model using update()\nlm.fit1 &lt;- update(lm.fit, ~ . - age)\n\n\n\nInteraction terms\nThere are two ways to include interaction terms in the lm() funtion: \\(x_1:x_2\\) creates an interaction term between the two variables. \\(x_1 * x_2\\) creates an individual variable for each plus an interaction term.\n(\\(x_1 * x_2\\) is shorthand for \\(x_1 + x_2 + x_1:x_2\\))\n\n#Run a regression with a predictor variable\nsummary(lm(medv ~ lstat*age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nNon-linear transformations on predictors\nTo transform a variable in lm(), use I(). For example, to square a predictor you would use I(x^2).\nFor higher order variables, use the poly() function.\n\n#Run a regression with a squared predictor term\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n#Use anova() to see if the quadratic fit is better than the original linear fit\nlm.fit &lt;- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow = c(2,2))\nplot(lm.fit2)\n\n\n\n#Use the poly() function within lm() to generate a regression with higher order polynomials\nlm.fit5 &lt;- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n#Generate a regression with a log-transformed variable\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQualitative predictors\nlm() automatically generates dummy variables in a regression on a dataset with qualitative variables.\n\n#Preview the Carseats dataset with information about carseat sales\nstr(Carseats)\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n#A regression on Carseats data with all variables and some interaction terms\nlm.fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n#Use contrasts() to view the coding scheme for a qualitative variable\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\nWriting functions in R\n\n#Write a function to load relevant libraries\nLoadLibraries &lt;- function() {\n  library(ISLR2)\n  library(MASS)\n  print(\"The libraries have been loaded.\")\n}"
  },
  {
    "objectID": "index.html#chapter-4-classification",
    "href": "index.html#chapter-4-classification",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 4: Classification",
    "text": "Chapter 4: Classification\n\nNotes\nClassification techniques are used when the dependent variable Y is qualitative or categorical.\n\nLogistic regression\n\nEstimation process:\n\nCreate probability functions to estimate coefficients based on training data\nUse new probability equation to predict probabilities\nSet a boundary (i.e. .5) to classy to one category or the other\ninterpreting coefficients - change in log-odds, not change in probability\n\nNote: the coefficients estimated in logistic regression are the change in log-odds, however, these can be plugged back into an equation to calculate probability\n\nBayes Classifier\n\nCalculate probabilities of each classification outcome\nBayes classifier picks the largest probability to assign a class\nWhat kind of data is needed to calculate probabilities in step 1? You need a lot of it to make accurate calculations, and sufficient data is not often available. It can, however, be used with simulated data for examples.\n\nLinear Discriminant Analysis (LDA)\n\nBayes Theorem - used to flip conditional probabilities\nAssumption - the distributions of X are normal or Gaussian\nAssumption - both the outcome distributions have the same variance (not an assumption of QDA)\nData statistics needed to plug into the equation with two classes and one predictor: \\(\\sigma, \\pi_1, \\pi_2, \\mu_1, \\mu_2\\) where \\(\\sigma\\) is the shared variance, \\(\\pi_1\\) and \\(\\pi_2\\) are the prior probabilities of k = 1, 2, and \\(\\mu_1\\) and \\(\\mu_2\\) are the means\n\nQuadratic Discriminant Analysis (QDA)\nNaive Bayes\n\nComparison of methods\n\n\nLab: Classification Models in R\nSummary of functions in R: lm() Linear regression glm(...family = binomial) Logistic regression lda() Linear Discriminant Analysis (from the MASS package) qda() Quadratic Discriminant Analysis (from the MASS package) naiveBayes() Naive Bayes (from the e1071 package)\n\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\ncor(Smarket[,-9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n\nattach(Smarket)\nplot(Volume)\n\n\n\n\n\nLogistic Regression\nThe glm() function can be used for generalized linear models including logistic regression. Need to include the argument family = binomial to specify logistic regression as the method.\n\n#Construct a logistic regression model to predict the qualitative variable Direction\nglm.fits &lt;- glm(\n  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial\n)\n\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n#View the coefficients\ncoef(glm.fits)\n\n (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n      Volume \n 0.135440659 \n\n#Another way to view the coefficients\nsummary(glm.fits)$coef\n\n                Estimate Std. Error    z value  Pr(&gt;|z|)\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\n\n#Access just the p-values for the coefficients\nsummary(glm.fits)$coef[,4]\n\n(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 \n  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 \n     Volume \n  0.3924004 \n\n#Predict the probability that the market will go up or down\n#Set type = response to output probabilities of the form P(Y = 1|X)\nglm.probs &lt;- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\n#Check whether the probability is of the market going up or down (it is Up)\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n#Create a vector of predicted outcomes (up or down)\nglm.pred &lt;- rep(\"Down\", 1250)\nglm.pred[glm.probs &gt; .5] = \"Up\"\n\n#Generate a confusion matrix to see the correct and incorrect predictions\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n\nWhen building and testing models it is useful to split the data into training data and test data. This way you can see well your model will work when exposed to new data.\nStep 1: Train the model\n\n#Create vector of all observations before 2005\ntrain &lt;- (Year &lt; 2005)\n#Use vector to create a dataset of observations only from 2005,\n#Boolean vectors can be used to obtain a subset of the rows or columns of a matrix.\nSmarket.2005 &lt;- Smarket[!train,]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\n#Alternate method of filtering the data\nSmarket.2005b &lt;- subset(Smarket, Year &gt;= 2005)\n\n#Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.\nglm.fits &lt;- glm(\n  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n  data = Smarket, family = binomial, subset = train\n)\n\nStep 2: Test the model with new data\n\n#Predict direction of the stock market for only the data in 2005\nglm.probs &lt;- predict(glm.fits, Smarket.2005, type = \"response\")\n\nStep 3: Compare the predictions to the actual outcomes in the test data\n\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n#Test set success rate\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\n#Test set error rate\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe resulting error rate is not very good but not a surprising result for trying to predict future stock market conditions only based on the last few days. Below the analysis is repeated, but only keeping the predictors Lag 1 and Lag 2 which had the smallest P-values (i.e. higher significance)\n\nglm.fits &lt;- glm(\n  Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train\n)\nglm.probs &lt;- predict(glm.fits, Smarket.2005, type = \"response\")\nglm.pred &lt;- rep(\"Down\",250)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n\nPredictions can also be modified to use differenct coefficient values:\n\npredict(glm.fits,\n        newdata = \n          data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),\n        type = \"response\"\n)\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\nLinear Discriminant Analysis\n\nlibrary(MASS)\n#Set up a Linear Discriminant Analysis model\nlda.fit &lt;- lda(\n  Direction ~ Lag1 + Lag2, data = Smarket, subset = train\n)\n\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit)\n\n\n\nlda.pred &lt;- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nThe predict() function returns a list with three elements:\n\nclass LDA’s predictions about the movement of the market\nposterior a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class\nx contains the linear discriminants\n\n\nlda.class &lt;- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nIn this case, LDA produced similar results to Logistic Regression\n\nsum(lda.pred$posterior[,1] &gt;= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[,1] &lt; .5)\n\n[1] 182\n\nlda.pred$posterior[1:20,1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n#Adjust threshold above .5\nsum(lda.pred$posterior[,1] &gt; .9)\n\n[1] 0\n\n\n\n\nQuadratic Discriminant Analysis\n\n#Set up a Quadratic Discriminant Analysis model\nqda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nqda.class &lt;- predict(qda.fit, Smarket.2005)$class\n\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\n\n\nNaive Bayes\n\n#load library containing the naiveBayes() function\nlibrary(e1071)\n\n#Create Naive Bayes model\nnb.fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n#Use model for prediction\nnb.class &lt;- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds &lt;- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5,]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "index.html#chapter-5-resampling-methods",
    "href": "index.html#chapter-5-resampling-methods",
    "title": "Intro to Machine Learning Notes",
    "section": "Chapter 5: Resampling Methods",
    "text": "Chapter 5: Resampling Methods\n\nNotes\nResampling methods: drawing samples from the training data to fit your model multiple times, comparing the goodness of fit across each sample.\n\nModel assessment - the process of evaluating a model’s performance\nModel selection - the process of the proper level of flexibility for a model\n\nCross-validation\nValidation Set approach\n\nIf a test dataset is not available, you can split the training data into two sets:\n\ntraining set\nvalidation set\n\nFit the model using the training set, and then test it on the validation set.\n\nYou can also do this multiple times by using different random selections for the training and validation sets\n\nCreating a validation set is a useful and simple approach but has two drawbacks:\n\nThe validation estimate of the test error can vary substantially depending on how the data is split up\nOnly a subset of observations are used to train the model, since some must be withheld for the test/validation set.\n\n\nLeave One Out Cross Validation (LOOCV)\n\nSimilar to the validation set approach, but instead of splitting the observations in half, simply split one observation off for validation and use the rest (n-1) for the training set\nRepeat the procedure n times, calculating the MSE each time\nLOOCV estimate is the mean of all the MSEs\nDownside - has to be fit n times which can be time consuming or expensive\n\nA special rule allows a shortcut for linear or polynomial regression, needing only one fit\n\n\nk-Fold Cross Validation\n\nDivide the observations into k folds of equal size, using one for validation and the rest for training\nRepeat the procedure k times using a different fold as the validation set, recording MSE\nTake the average of all the MSEs\nTypically performed using k = 5 or k = 10\n\nCross validation with classification\n\nSame as above, but instead of using MSE to quantify test error, use the number of misclassified observations\n\n\nBootstrap\n\n\nLab: Cross Validation and the Bootstrap\n\nValidation Set Approach\n\nlibrary(ISLR2)\nset.seed(1)\n\n#Split data in two for training and testing\ntrain &lt;- sample(392, 196)\n\nlm.fit &lt;- lm(mpg ~ horsepower, data = Auto, subset = train)\n\nattach(Auto)\n\nThe following object is masked from package:lubridate:\n\n    origin\n\n\nThe following object is masked from package:ggplot2:\n\n    mpg\n\nmean((mpg - predict(lm.fit, Auto))[-train]^2)\n\n[1] 23.26601\n\n\n\n\nLeave One Out Cross Validation\n\n\nk-fold Cross Validation\n\n\nBootstrap"
  }
]