tree.carseats <- tree(High ~ . -Sales, Carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . -Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#| output = FALSE
#Load packages for chapter labs
library("ISLR2") #Introduction to Statistical Learning datasets'
library("tidyverse")
x <- seq(1,10)
x
x <- 1:10
x
x <- seq(-pi, pi, length = 50)
y <- x
f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))
contour(x, y, f)
contour(x, y, f, nlevels = 45, add = T)
fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta = 30)
persp(x, y, fa, theta = 30, phi = 20)
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
library(MASS)
#view the first 10 observations of the dataset
head(Boston)
#Create a regression equation
attach(Boston)
lm.fit <- lm(medv ~ lstat)
#View the regression results
lm.fit
#View details about the regression
summary(lm.fit)
#See what is stored in the lm.fit list
names(lm.fit)
#Function to view the coefficients of lm.fit
coef(lm.fit)
#View the confidence interval
confint(lm.fit)
#Generate confidence intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
#Generate prediction intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
#plot
plot(lstat, medv)
#Add the least squares line to the plot
abline(lm.fit, lwd = 3, col = "red")
#Use 'col =' to change the color of the points
plot(lstat, medv, col = "red")
#Use 'pch =' to change the shape of the points
plot(lstat, medv, pch = 20)
#Define the point shape directly
plot(lstat, medv, pch = "+")
#Define the point shape with a number
plot(1:20, 1:20, pch = 1:20)
par(mfrow = c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
#Run a regression with specified predictors
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
#Run a regression on all the predictor variables in the dataset
lm.fit <- lm(formula = medv ~ ., data = Boston)
summary(lm.fit)
library(car)
#Calculate variance inflation factors
vif(lm.fit)
#Run the regression all predictors except one (age) using the "-" sign
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
#Another way to change the model using update()
lm.fit1 <- update(lm.fit, ~ . - age)
#Run a regression with a predictor variable
summary(lm(medv ~ lstat*age, data = Boston))
#Run a regression with a squared predictor term
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
#Use anova() to see if the quadratic fit is better than the original linear fit
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
par(mfrow = c(2,2))
plot(lm.fit2)
#Use the poly() function within lm() to generate a regression with higher order polynomials
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
#Generate a regression with a log-transformed variable
summary(lm(medv ~ log(rm), data = Boston))
#Preview the Carseats dataset with information about carseat sales
str(Carseats)
#A regression on Carseats data with all variables and some interaction terms
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)
#Use contrasts() to view the coding scheme for a qualitative variable
attach(Carseats)
contrasts(ShelveLoc)
#Write a function to load relevant libraries
LoadLibraries <- function() {
library(ISLR2)
library(MASS)
print("The libraries have been loaded.")
}
names(Smarket)
dim(Smarket)
summary(Smarket)
cor(Smarket[,-9])
attach(Smarket)
plot(Volume)
#Construct a logistic regression model to predict the qualitative variable Direction
glm.fits <- glm(
Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial
)
summary(glm.fits)
#View the coefficients
coef(glm.fits)
#Another way to view the coefficients
summary(glm.fits)$coef
#Access just the p-values for the coefficients
summary(glm.fits)$coef[,4]
#Predict the probability that the market will go up or down
#Set type = response to output probabilities of the form P(Y = 1|X)
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
#Check whether the probability is of the market going up or down (it is Up)
contrasts(Direction)
#Create a vector of predicted outcomes (up or down)
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
#Generate a confusion matrix to see the correct and incorrect predictions
table(glm.pred, Direction)
#Create vector of all observations before 2005
train <- (Year < 2005)
#Use vector to create a dataset of observations only from 2005,
#Boolean vectors can be used to obtain a subset of the rows or columns of a matrix.
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
#Alternate method of filtering the data
Smarket.2005b <- subset(Smarket, Year >= 2005)
#Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.
glm.fits <- glm(
Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = Smarket, family = binomial, subset = train
)
#Predict direction of the stock market for only the data in 2005
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
#Test set success rate
mean(glm.pred == Direction.2005)
#Test set error rate
mean(glm.pred != Direction.2005)
glm.fits <- glm(
Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train
)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down",250)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
predict(glm.fits,
newdata =
data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
type = "response"
)
library(MASS)
#Set up a Linear Discriminant Analysis model
lda.fit <- lda(
Direction ~ Lag1 + Lag2, data = Smarket, subset = train
)
lda.fit
plot(lda.fit)
lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
sum(lda.pred$posterior[,1] >= .5)
sum(lda.pred$posterior[,1] < .5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
#Adjust threshold above .5
sum(lda.pred$posterior[,1] > .9)
#Set up a Quadratic Discriminant Analysis model
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda.fit
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
#load library containing the naiveBayes() function
library(e1071)
#Create Naive Bayes model
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
nb.fit
#Use model for prediction
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)
mean(nb.class == Direction.2005)
nb.preds <- predict(nb.fit, Smarket.2005, type = "raw")
nb.preds[1:5,]
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
#Fit a quadratic and cubic model with the poly() function
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg- predict(lm.fit3, Auto))[-train]^2)
#boot package contains cv.glm()
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
cv.error <- rep(0,10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
#Display the test MSE for each all polynomial models
cv.error
set.seed(17)
cv.error.10 <- rep(0,10)
for(i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
alpha.fn <- function(data, index){
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y))
}
alpha.fn(Portfolio, 1:100)
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = TRUE))
boot(
data = Portfolio,
statistic = alpha.fn,
R = 1000
)
#Define function (no brackets needed since it's only one line)
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
#
boot.fn(Auto, 1:392)
set.seed(1)
boot.fn(Auto, sample(392, 392, TRUE))
boot(Auto, boot.fn, 1000)
summary(lm(mpg ~ horsepower, data = Auto))$coef
#Bootstrap method
boot.fn <- function(data, index)
coef(
lm(mpg ~ horsepower + I(horsepower^2),
data = data, subset = index)
)
set.seed(1)
boot(Auto, boot.fn, 1000)
#standard calculation
summary(
lm(mpg ~ horsepower + I(horsepower^2), Auto)
)$coef
#Preview the data
names(Hitters)
dim(Hitters)
str(Hitters)
#Find the number of missing values for Salary
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
library(leaps)
regfit.full <- regsubsets(Salary~., Hitters)
summary(regfit.full)
regfit.full <- regsubsets(Salary~., Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
names(reg.summary)
#view the R^2 statistic
reg.summary$rsq
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2,
pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2,
pch = 20)
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
coef(regfit.full, 6)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . -Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
high <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, high)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(high ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
high <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, high)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(high ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(Carseats$High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(Carseats$High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(Carseats$High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#Load package for decision trees
library(tree)
library(ISLR2)
#Load the Carseats sales dataset for analysis
attach(Carseats)
#Create new variable to classify sales as High or low
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
#Combine High variable with Carseats data
Carseats <- data.frame(Carseats, High)
#Create classification tree model to predict High with all variables except Sales
tree.carseats <- tree(High ~ . - Sales, Carseats)
#List the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
