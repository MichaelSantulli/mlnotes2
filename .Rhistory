false_neg <- qda.matrix[1,2]/(qda.matrix[1,2]+qda.matrix[2,2])
error_rate <- (qda.matrix[2,1]+qda.matrix[1,2])/length(Weekly_Test$Direction)
#Create parameters for the knn() function
#Training data predictors
train.X <- cbind(Lag1, Lag2)[train,]
#train.X <- Lag2[train]
#Test data predictors
test.X <- cbind(Lag1, Lag2)[!train,]
#test.X <- Lag2[!train]
#Training data outcomes
train.Direction <- Direction[train]
#Set random seed for reproduceability
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
knn.matrix <- table(knn.pred, Weekly_Test$Direction)
knn.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- knn.matrix[2,1]/(knn.matrix[1,1]+knn.matrix[2,1])
false_neg <- knn.matrix[1,2]/(knn.matrix[1,2]+knn.matrix[2,2])
error_rate <- (knn.matrix[2,1]+knn.matrix[1,2])/length(Weekly_Test$Direction)
nb.fit <- naiveBayes(
Direction ~ Lag2,
data = Weekly_Train
)
summary(nb.fit)
nb.class <- predict(nb.fit, Weekly_Test)
nb.matrix <- table(nb.class, Weekly_Test$Direction)
nb.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- nb.matrix[2,1]/(nb.matrix[1,1]+nb.matrix[2,1])
false_neg <- nb.matrix[1,2]/(nb.matrix[1,2]+nb.matrix[2,2])
error_rate <- (nb.matrix[2,1]+nb.matrix[1,2])/length(Weekly_Test$Direction)
#Create new logistic regression model with an interaction term between Lag 2 and Volume
glm.fit <- glm(
Direction ~ Lag2*Volume,
data = Weekly_Train,
family = binomial
)
summary(glm.fit)
#Predict the outcome on the test data
glm.probs <- predict(glm.fit, Weekly_Test, type = "response")
glm.pred <- rep("Down", length(Weekly_Test$Direction))
glm.pred[glm.probs2 > .5] = "Up"
glm.matrix <- table(glm.pred, Weekly_Test$Direction)
glm.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- glm.matrix[2,1]/(glm.matrix[1,1]+glm.matrix[2,1])
false_neg <- glm.matrix[1,2]/(glm.matrix[1,2]+glm.matrix[2,2])
error_rate <- (glm.matrix[2,1]+glm.matrix[1,2])/length(Weekly_Test$Direction)
#Create new linear discriminant analysis (LDA) model
lda.fit <- lda(
Direction ~ Lag2 + Volume,
data = Weekly_Train
)
summary(lda.fit)
#Predict the outcome on the test data
lda.pred <- predict(lda.fit, Weekly_Test)
lda.class <- lda.pred$class
lda.matrix <- table(lda.class, Weekly_Test$Direction)
lda.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- lda.matrix[2,1]/(lda.matrix[1,1]+lda.matrix[2,1])
false_neg <- lda.matrix[1,2]/(lda.matrix[1,2]+lda.matrix[2,2])
error_rate <- (lda.matrix[2,1]+lda.matrix[1,2])/length(Weekly_Test$Direction)
#Create a new quadratic discriminant analysis (QDA) model
qda.fit <- qda(
Direction ~ Lag2 + I(Lag2^2),
data = Weekly_Train
)
summary(qda.fit)
#Predict the outcome on the test data
qda.pred <- predict(qda.fit, Weekly_Test)
qda.class <- qda.pred$class
qda.matrix <- table(qda.class, Weekly_Test$Direction)
qda.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- qda.matrix[2,1]/(qda.matrix[1,1]+qda.matrix[2,1])
false_neg <- qda.matrix[1,2]/(qda.matrix[1,2]+qda.matrix[2,2])
error_rate <- (qda.matrix[2,1]+qda.matrix[1,2])/length(Weekly_Test$Direction)
#Create a new quadratic discriminant analysis (QDA) model
qda.fit <- qda(
Direction ~ Lag2*Volume,
data = Weekly_Train
)
summary(qda.fit)
#Predict the outcome on the test data
qda.pred <- predict(qda.fit, Weekly_Test)
qda.class <- qda.pred$class
qda.matrix <- table(qda.class, Weekly_Test$Direction)
qda.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- qda.matrix[2,1]/(qda.matrix[1,1]+qda.matrix[2,1])
false_neg <- qda.matrix[1,2]/(qda.matrix[1,2]+qda.matrix[2,2])
error_rate <- (qda.matrix[2,1]+qda.matrix[1,2])/length(Weekly_Test$Direction)
knn.pred <- knn(train.X, test.X, train.Direction, k = 3)
knn.matrix <- table(knn.pred, Weekly_Test$Direction)
knn.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- knn.matrix[2,1]/(knn.matrix[1,1]+knn.matrix[2,1])
false_neg <- knn.matrix[1,2]/(knn.matrix[1,2]+knn.matrix[2,2])
error_rate <- (knn.matrix[2,1]+knn.matrix[1,2])/length(Weekly_Test$Direction)
#Calculate false positive, false negative, and total error rates
false_pos <- knn.matrix[2,1]/(knn.matrix[1,1]+knn.matrix[2,1])
false_neg <- knn.matrix[1,2]/(knn.matrix[1,2]+knn.matrix[2,2])
error_rate <- (knn.matrix[2,1]+knn.matrix[1,2])/length(Weekly_Test$Direction)
#Preview the dataset
glimpse(Auto)
#Calculate median MPG
mpg_median <- median(Auto$mpg)
#Create new column to show if mpg is greater or less than median
mpg_01 <- rep(0,length(Auto$mpg))
mpg_01[Auto$mpg > mpg_median] = 1
#Add new column to Auto dataset
Auto2 <- cbind(mpg_01,Auto)
pairs(Auto2)
#I didn't want to split the dataset by any of the existing variables since it seemed like they should be distributed randomly in both train and test, so I took the following approach:
#Create a vector of random numbers between 0 and 1
set.seed(1)
random <- runif(length(Auto2$mpg_01),0,1)
#Add the new column to the dataset so every observation has a random number
Auto2 <- cbind(Auto2, random)
#Split the dataset based on the random number values
Auto2_Train <- subset(Auto2, random < 0.5)
Auto2_Test  <- subset(Auto2, random >= 0.5)
lda.fit <- lda(
mpg_01 ~ horsepower + weight + acceleration,
data = Auto2_Train
)
lda.pred <- predict(lda.fit, Auto2_Test)
lda.class <- lda.pred$class
lda.matrix <- table(lda.class, Auto2_Test$mpg_01)
lda.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- lda.matrix[2,1]/(lda.matrix[1,1]+lda.matrix[2,1])
false_neg <- lda.matrix[1,2]/(lda.matrix[1,2]+lda.matrix[2,2])
error_rate <- (lda.matrix[2,1]+lda.matrix[1,2])/length(Auto2_Test$mpg_01)
qda.fit <- qda(
mpg_01 ~ horsepower + weight + acceleration,
data = Auto2_Train
)
qda.pred <- predict(qda.fit, Auto2_Test)
qda.class <- qda.pred$class
qda.matrix <- table(qda.class, Auto2_Test$mpg_01)
qda.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- qda.matrix[2,1]/(qda.matrix[1,1]+qda.matrix[2,1])
false_neg <- qda.matrix[1,2]/(qda.matrix[1,2]+qda.matrix[2,2])
error_rate <- (qda.matrix[2,1]+qda.matrix[1,2])/length(Auto2_Test$mpg_01)
glm.fit <- glm(
mpg_01 ~ horsepower + weight + acceleration,
data = Auto2_Train,
family = binomial
)
glm.probs <- predict(glm.fit, Auto2_Test, type = "response")
glm.pred <- rep(0,length(Auto2_Test$mpg_01))
glm.pred[glm.probs > .5] = 1
glm.matrix <- table(glm.pred, Auto2_Test$mpg_01)
glm.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- glm.matrix[2,1]/(glm.matrix[1,1]+glm.matrix[2,1])
false_neg <- glm.matrix[1,2]/(glm.matrix[1,2]+glm.matrix[2,2])
error_rate <- (glm.matrix[2,1]+glm.matrix[1,2])/length(Auto2_Test$mpg_01)
nb.fit <- naiveBayes(
mpg_01 ~ horsepower + weight + acceleration,
data = Auto2_Train
)
nb.class <- predict(nb.fit, Auto2_Test)
nb.matrix <- table(nb.class, Auto2_Test$mpg_01)
nb.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- nb.matrix[2,1]/(nb.matrix[1,1]+nb.matrix[2,1])
false_neg <- nb.matrix[1,2]/(nb.matrix[1,2]+nb.matrix[2,2])
error_rate <- (nb.matrix[2,1]+nb.matrix[1,2])/length(Auto2_Test$mpg_01)
#Create new training and test datasets keeping only the columns needed for prediction
train.X <- subset(Auto2_Train, select = c(horsepower, weight, acceleration))
test.X <- subset(Auto2_Test, select = c(horsepower, weight, acceleration))
knn.pred <- knn(train.X, test.X, Auto2_Train$mpg_01, k = 1)
knn.matrix <- table(knn.pred, Auto2_Test$mpg_01)
knn.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- knn.matrix[2,1]/(knn.matrix[1,1]+knn.matrix[2,1])
false_neg <- knn.matrix[1,2]/(knn.matrix[1,2]+knn.matrix[2,2])
error_rate <- (knn.matrix[2,1]+knn.matrix[1,2])/length(Auto2_Train$mpg_01)
#Rerun KNN with K = 2
#Create new training and test datasets keeping only the columns needed for prediction
train.X <- subset(Auto2_Train, select = c(horsepower, weight, acceleration))
test.X <- subset(Auto2_Test, select = c(horsepower, weight, acceleration))
knn.pred <- knn(train.X, test.X, Auto2_Train$mpg_01, k = 2)
knn.matrix <- table(knn.pred, Auto2_Test$mpg_01)
knn.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- knn.matrix[2,1]/(knn.matrix[1,1]+knn.matrix[2,1])
false_neg <- knn.matrix[1,2]/(knn.matrix[1,2]+knn.matrix[2,2])
error_rate <- (knn.matrix[2,1]+knn.matrix[1,2])/length(Auto2_Train$mpg_01)
#Rerun KNN with K = 3
#Create new training and test datasets keeping only the columns needed for prediction
train.X <- subset(Auto2_Train, select = c(horsepower, weight, acceleration))
test.X <- subset(Auto2_Test, select = c(horsepower, weight, acceleration))
knn.pred <- knn(train.X, test.X, Auto2_Train$mpg_01, k = 3)
knn.matrix <- table(knn.pred, Auto2_Test$mpg_01)
knn.matrix
#Calculate false positive, false negative, and total error rates
false_pos <- knn.matrix[2,1]/(knn.matrix[1,1]+knn.matrix[2,1])
false_neg <- knn.matrix[1,2]/(knn.matrix[1,2]+knn.matrix[2,2])
error_rate <- (knn.matrix[2,1]+knn.matrix[1,2])/length(Auto2_Train$mpg_01)
2^100
I(2^100)
?I
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
view(train)
View(train)
library(tidyverse)
view(train)
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
#Fit a quadratic and cubic model with the poly() function
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
#Fit a quadratic and cubic model with the poly() function
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
#Fit a quadratic and cubic model with the poly() function
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg- predict(lm.fit3, Auto))[-train]^2)
?sample
#| output = FALSE
#Load packages for chapter labs
library("ISLR2") #Introduction to Statistical Learning datasets'
library("tidyverse")
x <- seq(1,10)
x
x <- 1:10
x
x <- seq(-pi, pi, length = 50)
y <- x
f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))
contour(x, y, f)
contour(x, y, f, nlevels = 45, add = T)
fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta = 30)
persp(x, y, fa, theta = 30, phi = 20)
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
library(MASS)
#view the first 10 observations of the dataset
head(Boston)
#Create a regression equation
attach(Boston)
lm.fit <- lm(medv ~ lstat)
#View the regression results
lm.fit
#View details about the regression
summary(lm.fit)
#See what is stored in the lm.fit list
names(lm.fit)
#Function to view the coefficients of lm.fit
coef(lm.fit)
#View the confidence interval
confint(lm.fit)
#Generate confidence intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
#Generate prediction intervals for given values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
#plot
plot(lstat, medv)
#Add the least squares line to the plot
abline(lm.fit, lwd = 3, col = "red")
#Use 'col =' to change the color of the points
plot(lstat, medv, col = "red")
#Use 'pch =' to change the shape of the points
plot(lstat, medv, pch = 20)
#Define the point shape directly
plot(lstat, medv, pch = "+")
#Define the point shape with a number
plot(1:20, 1:20, pch = 1:20)
par(mfrow = c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
#Run a regression with specified predictors
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
#Run a regression on all the predictor variables in the dataset
lm.fit <- lm(formula = medv ~ ., data = Boston)
summary(lm.fit)
library(car)
#Calculate variance inflation factors
vif(lm.fit)
#Run the regression all predictors except one (age) using the "-" sign
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
#Another way to change the model using update()
lm.fit1 <- update(lm.fit, ~ . - age)
#Run a regression with a predictor variable
summary(lm(medv ~ lstat*age, data = Boston))
#Run a regression with a squared predictor term
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
#Use anova() to see if the quadratic fit is better than the original linear fit
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
par(mfrow = c(2,2))
plot(lm.fit2)
#Use the poly() function within lm() to generate a regression with higher order polynomials
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
#Generate a regression with a log-transformed variable
summary(lm(medv ~ log(rm), data = Boston))
#Preview the Carseats dataset with information about carseat sales
str(Carseats)
#A regression on Carseats data with all variables and some interaction terms
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)
#Use contrasts() to view the coding scheme for a qualitative variable
attach(Carseats)
contrasts(ShelveLoc)
#Write a function to load relevant libraries
LoadLibraries <- function() {
library(ISLR2)
library(MASS)
print("The libraries have been loaded.")
}
names(Smarket)
dim(Smarket)
summary(Smarket)
cor(Smarket[,-9])
attach(Smarket)
plot(Volume)
#Construct a logistic regression model to predict the qualitative variable Direction
glm.fits <- glm(
Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial
)
summary(glm.fits)
#View the coefficients
coef(glm.fits)
#Another way to view the coefficients
summary(glm.fits)$coef
#Access just the p-values for the coefficients
summary(glm.fits)$coef[,4]
#Predict the probability that the market will go up or down
#Set type = response to output probabilities of the form P(Y = 1|X)
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
#Check whether the probability is of the market going up or down (it is Up)
contrasts(Direction)
#Create a vector of predicted outcomes (up or down)
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
#Generate a confusion matrix to see the correct and incorrect predictions
table(glm.pred, Direction)
#Create vector of all observations before 2005
train <- (Year < 2005)
#Use vector to create a dataset of observations only from 2005,
#Boolean vectors can be used to obtain a subset of the rows or columns of a matrix.
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
#Alternate method of filtering the data
Smarket.2005b <- subset(Smarket, Year >= 2005)
#Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.
glm.fits <- glm(
Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = Smarket, family = binomial, subset = train
)
#Predict direction of the stock market for only the data in 2005
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
#Test set success rate
mean(glm.pred == Direction.2005)
#Test set error rate
mean(glm.pred != Direction.2005)
glm.fits <- glm(
Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train
)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
glm.pred <- rep("Down",250)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
predict(glm.fits,
newdata =
data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
type = "response"
)
library(MASS)
#Set up a Linear Discriminant Analysis model
lda.fit <- lda(
Direction ~ Lag1 + Lag2, data = Smarket, subset = train
)
lda.fit
plot(lda.fit)
lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
sum(lda.pred$posterior[,1] >= .5)
sum(lda.pred$posterior[,1] < .5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
#Adjust threshold above .5
sum(lda.pred$posterior[,1] > .9)
#Set up a Quadratic Discriminant Analysis model
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda.fit
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
#load library containing the naiveBayes() function
library(e1071)
#Create Naive Bayes model
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
nb.fit
#Use model for prediction
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)
mean(nb.class == Direction.2005)
nb.preds <- predict(nb.fit, Smarket.2005, type = "raw")
nb.preds[1:5,]
library(ISLR2)
set.seed(1)
#Split data in two for training and testing
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
#Calculate the test MSE
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
#Fit a quadratic and cubic model with the poly() function
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg- predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg- predict(lm.fit3, Auto))[-train]^2)
#boot package contains cv.glm()
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
glimpse(cv.err)
cv.error <- rep(0,10)
cv.error
cv.error <- rep(0,10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error <- rep(0,10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
cv.error
}
cv.error <- rep(0,10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
